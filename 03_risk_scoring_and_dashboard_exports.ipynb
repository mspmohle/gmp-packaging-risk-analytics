{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846759f0-1b0f-413b-9236-d507c9fb23cf",
   "metadata": {},
   "source": [
    "# 03 — Risk Scoring & Dashboard Exports\n",
    "\n",
    "This notebook turns the engineered datasets and baseline models produced in **02_feature_engineering_and_labels.ipynb** into **business-ready outputs**:\n",
    "\n",
    "- **Risk scoring** at the appropriate operational level (asset-hour and asset-day)\n",
    "- A **Top-N daily alert feed** (budgeted triage: *top 5 assets/day*)\n",
    "- **Interpretability artifacts** that explain *why* an alert fired (top coefficient contributions)\n",
    "- Clean **dashboard export tables** suitable for BI tools (Power BI/Tableau/Looker) and API serving\n",
    "\n",
    "## Inputs (from Notebook 02)\n",
    "\n",
    "We expect one or more run folders under:\n",
    "\n",
    "`data/processed/feature_engineering/<RUN_ID>/`\n",
    "\n",
    "Where `<RUN_ID>` looks like `YYYYMMDDThhmmssZ`.  \n",
    "This notebook will automatically locate the **most recent run folder** that contains the required artifacts.\n",
    "\n",
    "Key artifacts used (when present):\n",
    "- `panel_asset_hour_future_incident.parquet`\n",
    "- `panel_preprocess.joblib`\n",
    "- `panel_feature_names.csv`\n",
    "- `panel_baseline_logreg_saga.joblib`\n",
    "- `panel_X_test.npz`, `panel_y_test.npy`, `panel_ids_test.parquet` (for test scoring and actionability exports)\n",
    "- (Optional) time-split / event-derived artifacts for asset-level context\n",
    "\n",
    "## Outputs (this notebook)\n",
    "\n",
    "Exports are written to a new folder:\n",
    "\n",
    "`data/processed/risk_scoring/<EXPORT_RUN_ID>/`\n",
    "\n",
    "Core outputs:\n",
    "- `alerts_top5_assets_per_day.csv` — daily triage list (top 5 assets/day)\n",
    "- `alerts_top5_assets_per_day_drivers_long.csv` — “why” for each alert (top contributions at peak hour)\n",
    "- `asset_day_scores.csv` — asset-day risk rollup\n",
    "- `asset_risk_scores.csv` — overall asset risk ranking\n",
    "- `DASHBOARD_EXPORTS.json` — machine-readable manifest of generated export files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6480121-2dc8-4146-9a09-c41750f4a194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/parallels/projects/gmp-packaging-risk-analytics\n",
      "FE_DIR: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/feature_engineering\n",
      "RS_DIR: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring\n",
      "\n",
      "Selected PANEL_RUN_DIR: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/feature_engineering/20251220T211621Z\n",
      "\n",
      "Panel run dir contents (selected key files):\n",
      "  ✅ panel_baseline_logreg_saga.joblib\n",
      "  ✅ panel_feature_names.csv\n",
      "  ✅ panel_asset_hour_future_incident.parquet\n",
      "  ✅ panel_ids_test.parquet\n",
      "  ✅ panel_X_test.npz\n",
      "  ✅ panel_y_test.npy\n",
      "\n",
      "EXPORT_RUN_ID: 20251220T223649Z\n",
      "EXPORT_DIR: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z\n",
      "\n",
      "Initialized manifest: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json\n",
      "{\n",
      "  \"export_run_id\": \"20251220T223649Z\",\n",
      "  \"created_utc\": \"2025-12-20T22:36:49.011693+00:00\",\n",
      "  \"panel_run_dir\": \"/home/parallels/projects/gmp-packaging-risk-analytics/data/processed/feature_engineering/20251220T211621Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 1 — Setup: imports, paths, run auto-discovery, and output directory\n",
    "#\n",
    "# Purpose:\n",
    "#   - Define project root and standard folders\n",
    "#   - Auto-locate the MOST RECENT feature-engineering RUN folder that contains\n",
    "#     the panel artifacts we need (robust to kernel restarts and OUT_DIR drift)\n",
    "#   - Create a fresh export run directory for this notebook's outputs\n",
    "#   - Establish a small set of helper utilities used throughout the notebook\n",
    "#\n",
    "# Expected:\n",
    "#   - Repo structure similar to:\n",
    "#       gmp-packaging-risk-analytics/\n",
    "#         src/\n",
    "#         data/\n",
    "#           processed/feature_engineering/<RUN_ID>/\n",
    "#         *.ipynb\n",
    "#\n",
    "# Outputs created:\n",
    "#   - EXPORT_DIR: data/processed/risk_scoring/<EXPORT_RUN_ID>/\n",
    "#============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Project root + canonical data folders\n",
    "# -----------------------------\n",
    "# Notebook lives at repo root in your current layout (00_, 01_, 02_, 03_ sit together).\n",
    "# If you move notebooks later, adjust ROOT detection.\n",
    "ROOT = Path.cwd()\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "FE_DIR = DATA_DIR / \"processed\" / \"feature_engineering\"     # where notebook 02 wrote run folders\n",
    "RS_DIR = DATA_DIR / \"processed\" / \"risk_scoring\"            # where *this* notebook writes exports\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"FE_DIR:\", FE_DIR)\n",
    "print(\"RS_DIR:\", RS_DIR)\n",
    "\n",
    "assert FE_DIR.exists(), f\"Missing feature engineering folder: {FE_DIR}. Did you run notebook 02?\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Helper: find most recent run folder that contains required artifacts\n",
    "# -----------------------------\n",
    "# We look for a run folder that contains panel model artifacts.\n",
    "# This avoids brittle dependence on a prior variable like OUT_DIR, and survives kernel restarts.\n",
    "REQUIRED_PANEL_FILES = [\n",
    "    \"panel_baseline_logreg_saga.joblib\",\n",
    "    \"panel_feature_names.csv\",\n",
    "]\n",
    "\n",
    "OPTIONAL_BUT_COMMON_PANEL_FILES = [\n",
    "    \"panel_asset_hour_future_incident.parquet\",\n",
    "    \"panel_ids_test.parquet\",\n",
    "    \"panel_X_test.npz\",\n",
    "    \"panel_y_test.npy\",\n",
    "]\n",
    "\n",
    "def find_latest_run_dir(base_dir: Path, required_files: list[str]) -> Path:\n",
    "    \"\"\"\n",
    "    Return the newest run directory under base_dir that contains all required_files.\n",
    "    Run dirs look like 'YYYYMMDDThhmmssZ'. We sort lexicographically which works for that format.\n",
    "    \"\"\"\n",
    "    if not base_dir.exists():\n",
    "        raise FileNotFoundError(f\"Base directory does not exist: {base_dir}\")\n",
    "\n",
    "    candidates = []\n",
    "    for p in sorted(base_dir.glob(\"20*T*Z\"), reverse=True):\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        if all((p / f).exists() for f in required_files):\n",
    "            candidates.append(p)\n",
    "\n",
    "    if not candidates:\n",
    "        # Show nearby directories to help debugging\n",
    "        nearby = [x.name for x in sorted(base_dir.glob(\"*\"))[:10]]\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find any feature-engineering run folder containing required panel artifacts.\\n\"\n",
    "            f\"Base dir: {base_dir}\\n\"\n",
    "            f\"Required: {required_files}\\n\"\n",
    "            f\"Example entries in base dir: {nearby}\"\n",
    "        )\n",
    "\n",
    "    return candidates[0]\n",
    "\n",
    "PANEL_RUN_DIR = find_latest_run_dir(FE_DIR, REQUIRED_PANEL_FILES)\n",
    "\n",
    "print(\"\\nSelected PANEL_RUN_DIR:\", PANEL_RUN_DIR)\n",
    "\n",
    "# Quick visibility for the human\n",
    "print(\"\\nPanel run dir contents (selected key files):\")\n",
    "for fn in REQUIRED_PANEL_FILES + OPTIONAL_BUT_COMMON_PANEL_FILES:\n",
    "    p = PANEL_RUN_DIR / fn\n",
    "    print(f\"  {'✅' if p.exists() else '— '} {fn}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Create a new export run directory for this notebook\n",
    "# -----------------------------\n",
    "EXPORT_RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "EXPORT_DIR = RS_DIR / EXPORT_RUN_ID\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nEXPORT_RUN_ID:\", EXPORT_RUN_ID)\n",
    "print(\"EXPORT_DIR:\", EXPORT_DIR)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Mini utility: write a JSON manifest of outputs (updated later)\n",
    "# -----------------------------\n",
    "EXPORT_MANIFEST_PATH = EXPORT_DIR / \"DASHBOARD_EXPORTS.json\"\n",
    "\n",
    "def update_manifest(manifest_path: Path, new_items: dict) -> None:\n",
    "    \"\"\"\n",
    "    Merge new_items into an on-disk JSON manifest (create if absent).\n",
    "    Values should be JSON-serializable.\n",
    "    \"\"\"\n",
    "    if manifest_path.exists():\n",
    "        payload = json.loads(manifest_path.read_text())\n",
    "    else:\n",
    "        payload = {}\n",
    "\n",
    "    payload.update(new_items)\n",
    "    manifest_path.write_text(json.dumps(payload, indent=2))\n",
    "\n",
    "# Initialize manifest with provenance\n",
    "update_manifest(EXPORT_MANIFEST_PATH, {\n",
    "    \"export_run_id\": EXPORT_RUN_ID,\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"panel_run_dir\": str(PANEL_RUN_DIR),\n",
    "})\n",
    "\n",
    "print(\"\\nInitialized manifest:\", EXPORT_MANIFEST_PATH)\n",
    "print(EXPORT_MANIFEST_PATH.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d93a44-14b3-44a6-9abd-d7eb01e4ae41",
   "metadata": {},
   "source": [
    "## ### What Cell 1 Just Did — Setup, Run Discovery, and Export Run Initialization\n",
    "\n",
    "This cell initialized the notebook’s working context and made the pipeline **restart-safe** by locating the correct upstream artifacts automatically.\n",
    "\n",
    "**Project + folder setup**\n",
    "- Confirmed the repository root as:  \n",
    "  `/home/parallels/projects/gmp-packaging-risk-analytics`\n",
    "- Set the feature-engineering runs directory to:  \n",
    "  `/home/parallels/projects/gmp-packaging-risk-analytics/data/processed/feature_engineering`\n",
    "- Set the risk-scoring export base directory to:  \n",
    "  `/home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring`\n",
    "\n",
    "**Auto-discovered the most recent valid panel run**\n",
    "- Selected the latest feature-engineering run folder containing the required panel artifacts:  \n",
    "  `/home/parallels/projects/gmp-packaging-risk-analytics/data/processed/feature_engineering/20251220T211621Z`\n",
    "- Verified the key inputs exist for downstream scoring and actionability:\n",
    "  - `panel_baseline_logreg_saga.joblib`\n",
    "  - `panel_feature_names.csv`\n",
    "  - `panel_asset_hour_future_incident.parquet`\n",
    "  - `panel_ids_test.parquet`\n",
    "  - `panel_X_test.npz`\n",
    "  - `panel_y_test.npy`\n",
    "\n",
    "**Created a new export run folder for this notebook**\n",
    "- Created a new export run id: `20251220T223649Z`\n",
    "- Created the export output folder:  \n",
    "  `/home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z`\n",
    "\n",
    "**Initialized the export manifest**\n",
    "- Wrote `DASHBOARD_EXPORTS.json` into the export folder with provenance linking this export run back to the selected panel run directory:\n",
    "  ```json\n",
    "  {\n",
    "    \"export_run_id\": \"20251220T223649Z\",\n",
    "    \"created_utc\": \"2025-12-20T22:36:49.011693+00:00\",\n",
    "    \"panel_run_dir\": \"/home/parallels/projects/gmp-packaging-risk-analytics/data/processed/feature_engineering/20251220T211621Z\"\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd47d487-ec85-499e-9323-818a87de1878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:\n",
      "  PANEL_RUN_DIR : /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/feature_engineering/20251220T211621Z\n",
      "  X_test_tx     : (8071, 76) | sparse: True\n",
      "  y_test        : (8071,) | pos_rate: 0.075208\n",
      "  ids_test      : (8071, 6)\n",
      "  ts_col        : ts_hour_utc\n",
      "\n",
      "Scored table snapshot:\n",
      "  rows          : 8,071\n",
      "  unique assets : 24\n",
      "  date range    : 2025-11-27 00:00:00+00:00 → 2025-12-11 00:00:00+00:00\n",
      "\n",
      "Saved: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_test_scores_asset_hour.parquet\n",
      "Saved: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_test_scores_asset_hour.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_id</th>\n",
       "      <th>ts_hour_utc</th>\n",
       "      <th>site_id</th>\n",
       "      <th>line_id</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>is_legacy</th>\n",
       "      <th>y_true</th>\n",
       "      <th>p_hat</th>\n",
       "      <th>ts_utc</th>\n",
       "      <th>date_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 00:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131399</td>\n",
       "      <td>2025-11-27 00:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 01:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.098103</td>\n",
       "      <td>2025-11-27 01:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 02:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.108845</td>\n",
       "      <td>2025-11-27 02:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 03:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.122167</td>\n",
       "      <td>2025-11-27 03:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 04:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.126038</td>\n",
       "      <td>2025-11-27 04:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 05:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.122089</td>\n",
       "      <td>2025-11-27 05:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 06:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111483</td>\n",
       "      <td>2025-11-27 06:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 07:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137937</td>\n",
       "      <td>2025-11-27 07:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 08:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.114635</td>\n",
       "      <td>2025-11-27 08:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A0001</td>\n",
       "      <td>2025-11-27 09:00:00+00:00</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.108846</td>\n",
       "      <td>2025-11-27 09:00:00+00:00</td>\n",
       "      <td>2025-11-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  asset_id               ts_hour_utc site_id line_id      asset_type  \\\n",
       "0    A0001 2025-11-27 00:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "1    A0001 2025-11-27 01:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "2    A0001 2025-11-27 02:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "3    A0001 2025-11-27 03:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "4    A0001 2025-11-27 04:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "5    A0001 2025-11-27 05:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "6    A0001 2025-11-27 06:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "7    A0001 2025-11-27 07:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "8    A0001 2025-11-27 08:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "9    A0001 2025-11-27 09:00:00+00:00      S1   S1-L2  blister_packer   \n",
       "\n",
       "   is_legacy  y_true     p_hat                    ts_utc    date_utc  \n",
       "0      False       0  0.131399 2025-11-27 00:00:00+00:00  2025-11-27  \n",
       "1      False       0  0.098103 2025-11-27 01:00:00+00:00  2025-11-27  \n",
       "2      False       0  0.108845 2025-11-27 02:00:00+00:00  2025-11-27  \n",
       "3      False       0  0.122167 2025-11-27 03:00:00+00:00  2025-11-27  \n",
       "4      False       0  0.126038 2025-11-27 04:00:00+00:00  2025-11-27  \n",
       "5      False       0  0.122089 2025-11-27 05:00:00+00:00  2025-11-27  \n",
       "6      False       0  0.111483 2025-11-27 06:00:00+00:00  2025-11-27  \n",
       "7      False       0  0.137937 2025-11-27 07:00:00+00:00  2025-11-27  \n",
       "8      False       0  0.114635 2025-11-27 08:00:00+00:00  2025-11-27  \n",
       "9      False       0  0.108846 2025-11-27 09:00:00+00:00  2025-11-27  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 2 — Load panel artifacts + score the TEST set\n",
    "#   Goal:\n",
    "#     - Load the trained panel baseline model + the TEST split artifacts\n",
    "#     - Produce a traceable scored table at the asset-hour level (with IDs)\n",
    "#     - Save the scored table as a canonical input for downstream rollups/exports\n",
    "#\n",
    "#   Inputs (from PANEL_RUN_DIR):\n",
    "#     - panel_X_test.npz\n",
    "#     - panel_y_test.npy\n",
    "#     - panel_ids_test.parquet\n",
    "#     - panel_feature_names.csv\n",
    "#     - panel_baseline_logreg_saga.joblib\n",
    "#\n",
    "#   Outputs (to EXPORT_DIR):\n",
    "#     - panel_test_scores_asset_hour.parquet\n",
    "#     - panel_test_scores_asset_hour.csv   (optional, smaller “business friendly” version)\n",
    "#============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Resolve input paths (panel run artifacts)\n",
    "# -----------------------------\n",
    "X_test_path   = Path(PANEL_RUN_DIR) / \"panel_X_test.npz\"\n",
    "y_test_path   = Path(PANEL_RUN_DIR) / \"panel_y_test.npy\"\n",
    "ids_test_path = Path(PANEL_RUN_DIR) / \"panel_ids_test.parquet\"\n",
    "feat_path     = Path(PANEL_RUN_DIR) / \"panel_feature_names.csv\"\n",
    "model_path    = Path(PANEL_RUN_DIR) / \"panel_baseline_logreg_saga.joblib\"\n",
    "\n",
    "for p in [X_test_path, y_test_path, ids_test_path, feat_path, model_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required panel artifact: {p}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load model + test matrices\n",
    "# -----------------------------\n",
    "X_test_tx = sp.load_npz(X_test_path)\n",
    "if not sp.issparse(X_test_tx):\n",
    "    # Defensive: should not happen, but keep robust if format changes.\n",
    "    X_test_tx = sp.csr_matrix(X_test_tx)\n",
    "else:\n",
    "    X_test_tx = X_test_tx.tocsr()\n",
    "\n",
    "y_test = np.load(y_test_path).astype(\"int8\")\n",
    "ids_test = pd.read_parquet(ids_test_path).copy()\n",
    "\n",
    "clf = joblib.load(model_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Load feature names (robust to schema)\n",
    "# -----------------------------\n",
    "feat_df = pd.read_csv(feat_path)\n",
    "if \"feature\" in feat_df.columns:\n",
    "    feature_names = feat_df[\"feature\"].astype(str).tolist()\n",
    "elif \"feature_name\" in feat_df.columns:\n",
    "    feature_names = feat_df[\"feature_name\"].astype(str).tolist()\n",
    "elif len(feat_df.columns) == 1:\n",
    "    feature_names = feat_df.iloc[:, 0].astype(str).tolist()\n",
    "else:\n",
    "    feature_names = feat_df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "# Sanity checks (prevents silent mismatches)\n",
    "nX = X_test_tx.shape[1]\n",
    "if len(feature_names) != nX:\n",
    "    raise ValueError(\n",
    "        \"Feature dimension mismatch:\\n\"\n",
    "        f\"  X_test features   : {nX}\\n\"\n",
    "        f\"  feature_names len : {len(feature_names)}\\n\"\n",
    "        f\"  feature file      : {feat_path}\"\n",
    "    )\n",
    "\n",
    "n_model = getattr(clf, \"n_features_in_\", None)\n",
    "if n_model is not None and n_model != nX:\n",
    "    raise ValueError(\n",
    "        \"Model feature dimension mismatch:\\n\"\n",
    "        f\"  X_test features : {nX}\\n\"\n",
    "        f\"  model expects   : {n_model}\\n\"\n",
    "        f\"  model file      : {model_path}\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Ensure key ID columns exist (traceability)\n",
    "# -----------------------------\n",
    "# We’ll treat these as “must-have” for dashboard exports.\n",
    "required_id_cols = [\"asset_id\"]\n",
    "missing = [c for c in required_id_cols if c not in ids_test.columns]\n",
    "if missing:\n",
    "    raise KeyError(\n",
    "        \"ids_test is missing required identifier columns:\\n\"\n",
    "        f\"  missing: {missing}\\n\"\n",
    "        f\"  columns: {ids_test.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "# Timestamp column is expected for “when” rollups.\n",
    "ts_col = None\n",
    "for c in [\"ts_hour_utc\", \"ts_hour\", \"timestamp_utc\", \"ts_utc\"]:\n",
    "    if c in ids_test.columns:\n",
    "        ts_col = c\n",
    "        break\n",
    "if ts_col is None:\n",
    "    raise KeyError(\n",
    "        \"Could not find a timestamp column in ids_test.\\n\"\n",
    "        f\"Available columns: {ids_test.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "# Parse timestamp consistently as UTC tz-aware\n",
    "ids_test[ts_col] = pd.to_datetime(ids_test[ts_col], utc=True, errors=\"coerce\")\n",
    "if ids_test[ts_col].isna().any():\n",
    "    bad = int(ids_test[ts_col].isna().sum())\n",
    "    raise ValueError(f\"{ts_col} has {bad} NaT values after parsing; check ids_test parquet.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Score the TEST set (hour-level)\n",
    "# -----------------------------\n",
    "# LogisticRegression supports predict_proba on sparse matrices.\n",
    "p_hat = clf.predict_proba(X_test_tx)[:, 1].astype(float)\n",
    "\n",
    "scores = ids_test.copy()\n",
    "scores[\"y_true\"] = y_test\n",
    "scores[\"p_hat\"] = p_hat\n",
    "scores[\"ts_utc\"] = scores[ts_col]  # canonical alias for downstream use\n",
    "scores[\"date_utc\"] = scores[\"ts_utc\"].dt.date\n",
    "\n",
    "# Quick sanity snapshot\n",
    "print(\"Loaded:\")\n",
    "print(f\"  PANEL_RUN_DIR : {PANEL_RUN_DIR}\")\n",
    "print(f\"  X_test_tx     : {X_test_tx.shape} | sparse: {sp.issparse(X_test_tx)}\")\n",
    "print(f\"  y_test        : {y_test.shape} | pos_rate: {float(y_test.mean()):.6f}\")\n",
    "print(f\"  ids_test      : {ids_test.shape}\")\n",
    "print(f\"  ts_col        : {ts_col}\")\n",
    "print(\"\\nScored table snapshot:\")\n",
    "print(f\"  rows          : {len(scores):,}\")\n",
    "print(f\"  unique assets : {scores['asset_id'].nunique():,}\")\n",
    "print(f\"  date range    : {scores['ts_utc'].min()} → {scores['ts_utc'].max()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Persist scored table (canonical downstream input)\n",
    "# -----------------------------\n",
    "out_parquet = Path(EXPORT_DIR) / \"panel_test_scores_asset_hour.parquet\"\n",
    "scores.to_parquet(out_parquet, index=False)\n",
    "print(\"\\nSaved:\", out_parquet)\n",
    "\n",
    "# Optional: export a smaller CSV (keeps it business-friendly)\n",
    "csv_cols = [c for c in [\"asset_id\", \"ts_utc\", \"date_utc\", \"p_hat\", \"y_true\", \"site_id\", \"line_id\", \"asset_type\", \"is_legacy\"] if c in scores.columns]\n",
    "out_csv = Path(EXPORT_DIR) / \"panel_test_scores_asset_hour.csv\"\n",
    "scores[csv_cols].to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n",
    "\n",
    "# Show a small preview\n",
    "display(scores.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4514e-10d0-4f21-8087-a5e944a99c25",
   "metadata": {},
   "source": [
    "### What Cell 2 Just Did\n",
    "\n",
    "This cell loaded the **panel model artifacts** from the selected feature-engineering run folder (`PANEL_RUN_DIR`) and produced a **traceable, hour-level risk scoring table** for the TEST split. Concretely, it loaded the sparse feature matrix `panel_X_test.npz` (shape **(8071, 76)**), the labels `panel_y_test.npy` (positive rate **0.075208**), the identifier table `panel_ids_test.parquet` (which includes `asset_id` and `ts_hour_utc`), the feature name list, and the trained baseline model `panel_baseline_logreg_saga.joblib`. It then computed **predicted probabilities** (`p_hat = P(target_future_incident=1)`) for every asset-hour in the TEST set, standardized the timestamp into a canonical `ts_utc` column, and added a daily bucket `date_utc` for downstream dashboard rollups.\n",
    "\n",
    "**Key outputs created in this run (`EXPORT_DIR`):**\n",
    "- `panel_test_scores_asset_hour.parquet` — the canonical scored TEST table (all columns, analysis-friendly)\n",
    "- `panel_test_scores_asset_hour.csv` — a smaller, business-friendly view (IDs + `p_hat` + label)\n",
    "\n",
    "**Sanity snapshot from the run output:**\n",
    "- Rows scored: **8,071** (asset-hours)\n",
    "- Unique test assets: **24**\n",
    "- UTC date range covered: **2025-11-27** through **2025-12-11**\n",
    "- The preview shows expected columns like: `asset_id`, `ts_hour_utc`/`ts_utc`, `site_id`, `line_id`, `asset_type`, `is_legacy`, `y_true`, and `p_hat`.\n",
    "\n",
    "These scored tables are the primary inputs for the next cells, where we’ll create **dashboard-ready rollups** (e.g., top risky assets, risk by day/site/line) and enforce an **alerts budget** such as “top 5 assets/day.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed874a84-5ee2-403e-88d6-3a4cdc4e6f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded scored TEST table:\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_test_scores_asset_hour.parquet\n",
      "  rows         : 8071\n",
      "  unique assets: 24\n",
      "  date range   : 2025-11-27 00:00:00+00:00 → 2025-12-11 00:00:00+00:00\n",
      "  pos rate (hour-level): 0.07520753314335274\n",
      "\n",
      "Alerts budget (TEST):\n",
      "  K (assets/day)          : 5\n",
      "  Avg assets/day selected : 5.00\n",
      "  Precision@K (asset-day) : 0.400\n",
      "\n",
      "Saved:\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_asset_day_scores_test.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_alerts_top5_assets_per_day_test.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_daily_rollup_test.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_site_day_rollup_test.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_line_day_rollup_test.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_alert_budget_top5_metrics_test.json\n",
      "\n",
      "Updated manifest: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json\n",
      "\n",
      "Top alerts (first 15 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_utc</th>\n",
       "      <th>asset_id</th>\n",
       "      <th>ts_peak</th>\n",
       "      <th>p_hat</th>\n",
       "      <th>y_asset_day</th>\n",
       "      <th>row_ix</th>\n",
       "      <th>site_id</th>\n",
       "      <th>line_id</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>is_legacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>S2</td>\n",
       "      <td>S2-L4</td>\n",
       "      <td>case_packer</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L4</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L1</td>\n",
       "      <td>cartoner</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0063</td>\n",
       "      <td>2025-11-27 16:00:00+00:00</td>\n",
       "      <td>0.692411</td>\n",
       "      <td>0</td>\n",
       "      <td>4389</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L1</td>\n",
       "      <td>sterilizer</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0037</td>\n",
       "      <td>2025-11-27 22:00:00+00:00</td>\n",
       "      <td>0.682174</td>\n",
       "      <td>0</td>\n",
       "      <td>2378</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L3</td>\n",
       "      <td>capper</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-28 11:00:00+00:00</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1</td>\n",
       "      <td>3400</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L4</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-28 13:00:00+00:00</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1</td>\n",
       "      <td>4074</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L1</td>\n",
       "      <td>cartoner</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0110</td>\n",
       "      <td>2025-11-28 04:00:00+00:00</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>1</td>\n",
       "      <td>7763</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L3</td>\n",
       "      <td>capper</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0019</td>\n",
       "      <td>2025-11-28 05:00:00+00:00</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1</td>\n",
       "      <td>1376</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L5</td>\n",
       "      <td>labeler</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0045</td>\n",
       "      <td>2025-11-28 14:00:00+00:00</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>1</td>\n",
       "      <td>3067</td>\n",
       "      <td>S2</td>\n",
       "      <td>S2-L2</td>\n",
       "      <td>labeler</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0074</td>\n",
       "      <td>2025-11-29 08:00:00+00:00</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>1</td>\n",
       "      <td>5774</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L4</td>\n",
       "      <td>vision_inspection</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0005</td>\n",
       "      <td>2025-11-29 03:00:00+00:00</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>1</td>\n",
       "      <td>388</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L2</td>\n",
       "      <td>environmental_monitor</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-29 22:00:00+00:00</td>\n",
       "      <td>0.791923</td>\n",
       "      <td>1</td>\n",
       "      <td>3435</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L4</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-29 17:00:00+00:00</td>\n",
       "      <td>0.787405</td>\n",
       "      <td>0</td>\n",
       "      <td>7127</td>\n",
       "      <td>S2</td>\n",
       "      <td>S2-L4</td>\n",
       "      <td>case_packer</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0063</td>\n",
       "      <td>2025-11-29 23:00:00+00:00</td>\n",
       "      <td>0.705877</td>\n",
       "      <td>0</td>\n",
       "      <td>4444</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L1</td>\n",
       "      <td>sterilizer</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_utc asset_id                   ts_peak     p_hat  y_asset_day  \\\n",
       "0   2025-11-27    A0105 2025-11-27 20:00:00+00:00  0.753193            0   \n",
       "1   2025-11-27    A0046 2025-11-27 18:00:00+00:00  0.731878            0   \n",
       "2   2025-11-27    A0056 2025-11-27 19:00:00+00:00  0.693739            0   \n",
       "3   2025-11-27    A0063 2025-11-27 16:00:00+00:00  0.692411            0   \n",
       "4   2025-11-27    A0037 2025-11-27 22:00:00+00:00  0.682174            0   \n",
       "5   2025-11-28    A0046 2025-11-28 11:00:00+00:00  0.999991            1   \n",
       "6   2025-11-28    A0056 2025-11-28 13:00:00+00:00  0.999987            1   \n",
       "7   2025-11-28    A0110 2025-11-28 04:00:00+00:00  0.999983            1   \n",
       "8   2025-11-28    A0019 2025-11-28 05:00:00+00:00  0.999980            1   \n",
       "9   2025-11-28    A0045 2025-11-28 14:00:00+00:00  0.999951            1   \n",
       "10  2025-11-29    A0074 2025-11-29 08:00:00+00:00  0.999956            1   \n",
       "11  2025-11-29    A0005 2025-11-29 03:00:00+00:00  0.999943            1   \n",
       "12  2025-11-29    A0046 2025-11-29 22:00:00+00:00  0.791923            1   \n",
       "13  2025-11-29    A0105 2025-11-29 17:00:00+00:00  0.787405            0   \n",
       "14  2025-11-29    A0063 2025-11-29 23:00:00+00:00  0.705877            0   \n",
       "\n",
       "    row_ix site_id line_id             asset_type  is_legacy  \n",
       "0     7082      S2   S2-L4            case_packer       True  \n",
       "1     3383      S4   S4-L4         blister_packer       True  \n",
       "2     4056      S3   S3-L1               cartoner       True  \n",
       "3     4389      S3   S3-L1             sterilizer       True  \n",
       "4     2378      S1   S1-L3                 capper       True  \n",
       "5     3400      S4   S4-L4         blister_packer       True  \n",
       "6     4074      S3   S3-L1               cartoner       True  \n",
       "7     7763      S4   S4-L3                 capper       True  \n",
       "8     1376      S3   S3-L5                labeler       True  \n",
       "9     3067      S2   S2-L2                labeler       True  \n",
       "10    5774      S3   S3-L4      vision_inspection       True  \n",
       "11     388      S4   S4-L2  environmental_monitor       True  \n",
       "12    3435      S4   S4-L4         blister_packer       True  \n",
       "13    7127      S2   S2-L4            case_packer       True  \n",
       "14    4444      S3   S3-L1             sterilizer       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daily rollup (preview):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_utc</th>\n",
       "      <th>n_hours</th>\n",
       "      <th>n_pos_hours</th>\n",
       "      <th>pos_rate_hours</th>\n",
       "      <th>p_hat_mean</th>\n",
       "      <th>p_hat_p95</th>\n",
       "      <th>p_hat_max</th>\n",
       "      <th>n_assets</th>\n",
       "      <th>n_alert_assets</th>\n",
       "      <th>n_alert_asset_days_positive</th>\n",
       "      <th>precision_at_k</th>\n",
       "      <th>max_alert_p_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>576</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393552</td>\n",
       "      <td>0.700848</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.753193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>576</td>\n",
       "      <td>78</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>0.797957</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>576</td>\n",
       "      <td>81</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.427053</td>\n",
       "      <td>0.734381</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.999956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>576</td>\n",
       "      <td>50</td>\n",
       "      <td>0.086806</td>\n",
       "      <td>0.544726</td>\n",
       "      <td>0.826490</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>576</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012153</td>\n",
       "      <td>0.551168</td>\n",
       "      <td>0.833077</td>\n",
       "      <td>0.864172</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.864172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>576</td>\n",
       "      <td>9</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.479401</td>\n",
       "      <td>0.781320</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-12-03</td>\n",
       "      <td>576</td>\n",
       "      <td>41</td>\n",
       "      <td>0.071181</td>\n",
       "      <td>0.398783</td>\n",
       "      <td>0.708626</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.999980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-12-04</td>\n",
       "      <td>576</td>\n",
       "      <td>46</td>\n",
       "      <td>0.079861</td>\n",
       "      <td>0.393049</td>\n",
       "      <td>0.712068</td>\n",
       "      <td>0.757414</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.757414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-12-05</td>\n",
       "      <td>576</td>\n",
       "      <td>13</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.799057</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.999978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-12-06</td>\n",
       "      <td>576</td>\n",
       "      <td>11</td>\n",
       "      <td>0.019097</td>\n",
       "      <td>0.422288</td>\n",
       "      <td>0.737457</td>\n",
       "      <td>0.777275</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_utc  n_hours  n_pos_hours  pos_rate_hours  p_hat_mean  p_hat_p95  \\\n",
       "0  2025-11-27      576            0        0.000000    0.393552   0.700848   \n",
       "1  2025-11-28      576           78        0.135417    0.499999   0.797957   \n",
       "2  2025-11-29      576           81        0.140625    0.427053   0.734381   \n",
       "3  2025-11-30      576           50        0.086806    0.544726   0.826490   \n",
       "4  2025-12-01      576            7        0.012153    0.551168   0.833077   \n",
       "5  2025-12-02      576            9        0.015625    0.479401   0.781320   \n",
       "6  2025-12-03      576           41        0.071181    0.398783   0.708626   \n",
       "7  2025-12-04      576           46        0.079861    0.393049   0.712068   \n",
       "8  2025-12-05      576           13        0.022569    0.499910   0.799057   \n",
       "9  2025-12-06      576           11        0.019097    0.422288   0.737457   \n",
       "\n",
       "   p_hat_max  n_assets  n_alert_assets  n_alert_asset_days_positive  \\\n",
       "0   0.753193        24               5                            0   \n",
       "1   0.999991        24               5                            5   \n",
       "2   0.999956        24               5                            3   \n",
       "3   0.999972        24               5                            2   \n",
       "4   0.864172        24               5                            0   \n",
       "5   0.999972        24               5                            1   \n",
       "6   0.999980        24               5                            3   \n",
       "7   0.757414        24               5                            1   \n",
       "8   0.999978        24               5                            1   \n",
       "9   0.777275        24               5                            0   \n",
       "\n",
       "   precision_at_k  max_alert_p_hat  \n",
       "0             0.0         0.753193  \n",
       "1             1.0         0.999991  \n",
       "2             0.6         0.999956  \n",
       "3             0.4         0.999972  \n",
       "4             0.0         0.864172  \n",
       "5             0.2         0.999972  \n",
       "6             0.6         0.999980  \n",
       "7             0.2         0.757414  \n",
       "8             0.2         0.999978  \n",
       "9             0.0         0.777275  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 3 — Dashboard rollups + alert budget (Top-5 assets/day on TEST)\n",
    "#   Build business-friendly summaries from the hour-level scored table:\n",
    "#     • Asset-day risk (max p_hat) + “when” (ts_peak) + asset-day truth label\n",
    "#     • Top-K assets/day alerts (K=5) + precision@K (asset-day)\n",
    "#     • Daily / Site-day / Line-day rollups for dashboard tiles\n",
    "#   Persist all outputs into EXPORT_DIR and register them in DASHBOARD_EXPORTS.json\n",
    "#============================================================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Load the scored TEST table produced in Cell 2\n",
    "# -----------------------------\n",
    "scores_path = EXPORT_DIR / \"panel_test_scores_asset_hour.parquet\"\n",
    "assert scores_path.exists(), f\"Missing {scores_path} (run Cell 2 first)\"\n",
    "\n",
    "scores = pd.read_parquet(scores_path).copy()\n",
    "\n",
    "required_cols = [\"asset_id\", \"p_hat\", \"y_true\"]\n",
    "for c in required_cols:\n",
    "    assert c in scores.columns, f\"Expected column '{c}' in scores table\"\n",
    "\n",
    "# Timestamp column: prefer ts_utc (canonical), fallback to ts_hour_utc if needed\n",
    "ts_col = \"ts_utc\" if \"ts_utc\" in scores.columns else (\"ts_hour_utc\" if \"ts_hour_utc\" in scores.columns else None)\n",
    "assert ts_col is not None, f\"Could not find timestamp column (ts_utc or ts_hour_utc). Columns: {scores.columns.tolist()}\"\n",
    "\n",
    "# Ensure tz-aware UTC timestamps\n",
    "scores[ts_col] = pd.to_datetime(scores[ts_col], utc=True, errors=\"coerce\")\n",
    "assert scores[ts_col].notna().all(), f\"{ts_col} has NaT after parsing; check upstream scoring.\"\n",
    "\n",
    "# Ensure date bucket exists (UTC)\n",
    "if \"date_utc\" not in scores.columns:\n",
    "    scores[\"date_utc\"] = scores[ts_col].dt.date\n",
    "\n",
    "# Optional context columns (present in your ids tables)\n",
    "context_cols = [c for c in [\"site_id\", \"line_id\", \"asset_type\", \"is_legacy\"] if c in scores.columns]\n",
    "\n",
    "print(\"Loaded scored TEST table:\")\n",
    "print(\" \", scores_path)\n",
    "print(\"  rows         :\", len(scores))\n",
    "print(\"  unique assets:\", scores[\"asset_id\"].nunique())\n",
    "print(\"  date range   :\", scores[ts_col].min(), \"→\", scores[ts_col].max())\n",
    "print(\"  pos rate (hour-level):\", float(scores[\"y_true\"].mean()))\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Asset-day risk table (max p_hat per asset per day)\n",
    "#    This gives a very business-friendly “top risky assets today” view.\n",
    "# -----------------------------\n",
    "# Keep a stable row index to track the “peak hour” row if we need it later\n",
    "scores = scores.reset_index(drop=True)\n",
    "scores[\"row_ix\"] = np.arange(len(scores), dtype=int)\n",
    "\n",
    "g_keys = [\"date_utc\", \"asset_id\"]\n",
    "\n",
    "# Find, for each asset-day: the row with the highest p_hat (peak hour)\n",
    "# We do this by sorting descending on p_hat then taking groupby().first()\n",
    "asset_day_peak = (\n",
    "    scores.sort_values(\"p_hat\", ascending=False)\n",
    "          .groupby(g_keys, as_index=False)\n",
    "          .first()\n",
    "          .rename(columns={ts_col: \"ts_peak\"})\n",
    ")\n",
    "\n",
    "# Asset-day truth label: did the asset have ANY positive hour that day?\n",
    "asset_day_truth = (\n",
    "    scores.groupby(g_keys, as_index=False)[\"y_true\"]\n",
    "          .max()\n",
    "          .rename(columns={\"y_true\": \"y_asset_day\"})\n",
    ")\n",
    "\n",
    "asset_day = asset_day_peak.merge(asset_day_truth, on=g_keys, how=\"left\")\n",
    "\n",
    "# Keep only the columns we want to expose on dashboards (plus useful context)\n",
    "keep_cols = (\n",
    "    [\"date_utc\", \"asset_id\", \"ts_peak\", \"p_hat\", \"y_asset_day\", \"row_ix\"]\n",
    "    + context_cols\n",
    ")\n",
    "asset_day = asset_day[[c for c in keep_cols if c in asset_day.columns]].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Enforce alerts budget: Top-5 assets/day (TEST)\n",
    "#    This is “precision@K” in an operational form: we only alert on 5 assets/day.\n",
    "# -----------------------------\n",
    "K = 5\n",
    "\n",
    "alerts = (\n",
    "    asset_day.sort_values([\"date_utc\", \"p_hat\"], ascending=[True, False])\n",
    "             .groupby(\"date_utc\", as_index=False)\n",
    "             .head(K)\n",
    "             .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Precision@K (asset-day): fraction of alerted asset-days with ≥1 positive hour\n",
    "precision_at_k = float(alerts[\"y_asset_day\"].mean()) if len(alerts) else np.nan\n",
    "avg_assets_per_day = float(alerts.groupby(\"date_utc\")[\"asset_id\"].nunique().mean()) if len(alerts) else 0.0\n",
    "\n",
    "print(\"\\nAlerts budget (TEST):\")\n",
    "print(f\"  K (assets/day)          : {K}\")\n",
    "print(f\"  Avg assets/day selected : {avg_assets_per_day:.2f}\")\n",
    "print(f\"  Precision@K (asset-day) : {precision_at_k:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Dashboard rollups (simple aggregations)\n",
    "#    These become easy KPI tiles / tables on a dashboard.\n",
    "# -----------------------------\n",
    "# Daily rollup: “how many high-risk predictions do we see each day?”\n",
    "daily = (\n",
    "    scores.groupby(\"date_utc\", as_index=False)\n",
    "          .agg(\n",
    "              n_hours=(\"p_hat\", \"size\"),\n",
    "              n_pos_hours=(\"y_true\", \"sum\"),\n",
    "              pos_rate_hours=(\"y_true\", \"mean\"),\n",
    "              p_hat_mean=(\"p_hat\", \"mean\"),\n",
    "              p_hat_p95=(\"p_hat\", lambda s: float(np.quantile(s, 0.95))),\n",
    "              p_hat_max=(\"p_hat\", \"max\"),\n",
    "              n_assets=(\"asset_id\", \"nunique\"),\n",
    "          )\n",
    ")\n",
    "\n",
    "# Add operational alert metrics (Top-5 assets/day)\n",
    "alerts_by_day = (\n",
    "    alerts.groupby(\"date_utc\", as_index=False)\n",
    "          .agg(\n",
    "              n_alert_assets=(\"asset_id\", \"nunique\"),\n",
    "              n_alert_asset_days_positive=(\"y_asset_day\", \"sum\"),\n",
    "              precision_at_k=(\"y_asset_day\", \"mean\"),\n",
    "              max_alert_p_hat=(\"p_hat\", \"max\"),\n",
    "          )\n",
    ")\n",
    "daily = daily.merge(alerts_by_day, on=\"date_utc\", how=\"left\")\n",
    "\n",
    "# Site-day rollup\n",
    "if \"site_id\" in scores.columns:\n",
    "    site_day = (\n",
    "        scores.groupby([\"date_utc\", \"site_id\"], as_index=False)\n",
    "              .agg(\n",
    "                  n_hours=(\"p_hat\", \"size\"),\n",
    "                  n_pos_hours=(\"y_true\", \"sum\"),\n",
    "                  pos_rate_hours=(\"y_true\", \"mean\"),\n",
    "                  p_hat_mean=(\"p_hat\", \"mean\"),\n",
    "                  p_hat_max=(\"p_hat\", \"max\"),\n",
    "                  n_assets=(\"asset_id\", \"nunique\"),\n",
    "              )\n",
    "    )\n",
    "else:\n",
    "    site_day = pd.DataFrame()\n",
    "\n",
    "# Line-day rollup\n",
    "if \"line_id\" in scores.columns:\n",
    "    line_day = (\n",
    "        scores.groupby([\"date_utc\", \"line_id\"], as_index=False)\n",
    "              .agg(\n",
    "                  n_hours=(\"p_hat\", \"size\"),\n",
    "                  n_pos_hours=(\"y_true\", \"sum\"),\n",
    "                  pos_rate_hours=(\"y_true\", \"mean\"),\n",
    "                  p_hat_mean=(\"p_hat\", \"mean\"),\n",
    "                  p_hat_max=(\"p_hat\", \"max\"),\n",
    "                  n_assets=(\"asset_id\", \"nunique\"),\n",
    "              )\n",
    "    )\n",
    "else:\n",
    "    line_day = pd.DataFrame()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Persist outputs + register in manifest\n",
    "# -----------------------------\n",
    "out_asset_day = EXPORT_DIR / \"panel_asset_day_scores_test.csv\"\n",
    "out_alerts    = EXPORT_DIR / \"panel_alerts_top5_assets_per_day_test.csv\"\n",
    "out_daily     = EXPORT_DIR / \"panel_daily_rollup_test.csv\"\n",
    "out_site_day  = EXPORT_DIR / \"panel_site_day_rollup_test.csv\"\n",
    "out_line_day  = EXPORT_DIR / \"panel_line_day_rollup_test.csv\"\n",
    "out_budget    = EXPORT_DIR / \"panel_alert_budget_top5_metrics_test.json\"\n",
    "\n",
    "asset_day.to_csv(out_asset_day, index=False)\n",
    "alerts.to_csv(out_alerts, index=False)\n",
    "daily.to_csv(out_daily, index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" \", out_asset_day)\n",
    "print(\" \", out_alerts)\n",
    "print(\" \", out_daily)\n",
    "\n",
    "if not site_day.empty:\n",
    "    site_day.to_csv(out_site_day, index=False)\n",
    "    print(\" \", out_site_day)\n",
    "if not line_day.empty:\n",
    "    line_day.to_csv(out_line_day, index=False)\n",
    "    print(\" \", out_line_day)\n",
    "\n",
    "budget_payload = {\n",
    "    \"policy\": \"top_k_assets_per_day\",\n",
    "    \"k\": int(K),\n",
    "    \"precision_at_k_asset_day_test\": float(precision_at_k) if precision_at_k == precision_at_k else None,\n",
    "    \"avg_assets_per_day_test\": float(avg_assets_per_day),\n",
    "    \"note\": \"precision@K computed on asset-day label y_asset_day=max(y_true) over hours for that asset on that UTC date (TEST only).\",\n",
    "}\n",
    "out_budget.write_text(json.dumps(budget_payload, indent=2))\n",
    "print(\" \", out_budget)\n",
    "\n",
    "# Update DASHBOARD_EXPORTS.json (append-only style)\n",
    "manifest_path = EXPORT_DIR / \"DASHBOARD_EXPORTS.json\"\n",
    "manifest = json.loads(manifest_path.read_text()) if manifest_path.exists() else {}\n",
    "manifest.setdefault(\"exports\", [])\n",
    "\n",
    "new_exports = [\n",
    "    out_asset_day.name,\n",
    "    out_alerts.name,\n",
    "    out_daily.name,\n",
    "    out_budget.name,\n",
    "]\n",
    "if not site_day.empty:\n",
    "    new_exports.append(out_site_day.name)\n",
    "if not line_day.empty:\n",
    "    new_exports.append(out_line_day.name)\n",
    "\n",
    "# De-dup while preserving order\n",
    "seen = set(manifest[\"exports\"])\n",
    "for f in new_exports:\n",
    "    if f not in seen:\n",
    "        manifest[\"exports\"].append(f)\n",
    "        seen.add(f)\n",
    "\n",
    "manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "print(\"\\nUpdated manifest:\", manifest_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Quick previews\n",
    "# -----------------------------\n",
    "print(\"\\nTop alerts (first 15 rows):\")\n",
    "display(alerts.head(15))\n",
    "\n",
    "print(\"\\nDaily rollup (preview):\")\n",
    "display(daily.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6319fcaa-cf95-4e4c-b555-73270f2642dd",
   "metadata": {},
   "source": [
    "### What Cell 3 Just Did\n",
    "\n",
    "This cell turns the hour-level panel test scores into **dashboard-ready rollups** and an **operational alert list** that respects our budget of **Top 5 assets per day**.\n",
    "\n",
    "**Inputs used**\n",
    "- Loaded the scored test set from Cell 2:  \n",
    "  `panel_test_scores_asset_hour.parquet` (8,071 rows, 24 assets)  \n",
    "- Confirmed the scoring time window covers **2025-11-27 → 2025-12-11 (UTC)** and the hour-level positive rate is **~0.0752**.\n",
    "\n",
    "**Outputs produced**\n",
    "1. **Asset-day scoring table (`panel_asset_day_scores_test.csv`)**\n",
    "   - Aggregated to one row per `(date_utc, asset_id)`\n",
    "   - Uses **max p_hat** across that day’s hours as the asset-day “risk”\n",
    "   - Captures **when** risk peaks via `ts_peak` (the hour with max p_hat)\n",
    "   - Adds an “asset-day truth” label `y_asset_day` = 1 if the asset had **any** positive hour that day\n",
    "\n",
    "2. **Alerts table with strict budget (`panel_alerts_top5_assets_per_day_test.csv`)**\n",
    "   - Enforces **K = 5 assets/day**\n",
    "   - For each day, selects the **top 5 assets** by asset-day max probability (`p_hat`)\n",
    "   - Computes **Precision@K (asset-day)** = fraction of alerted asset-days that had ≥1 positive hour\n",
    "\n",
    "3. **Dashboard KPI rollups**\n",
    "   - `panel_daily_rollup_test.csv`: day-level metrics (volume, pos rate, mean/p95/max p_hat, unique assets) plus the alert-budget performance per day\n",
    "   - `panel_site_day_rollup_test.csv`: same style rollup grouped by `(date_utc, site_id)`\n",
    "   - `panel_line_day_rollup_test.csv`: same style rollup grouped by `(date_utc, line_id)`\n",
    "\n",
    "4. **Budget policy metrics (`panel_alert_budget_top5_metrics_test.json`)**\n",
    "   - Records our alert policy (Top-K assets/day) and performance summary\n",
    "\n",
    "5. **Manifest update**\n",
    "   - Appended the new export filenames into:  \n",
    "     `DASHBOARD_EXPORTS.json`  \n",
    "   so downstream steps can discover and publish the latest export set automatically.\n",
    "\n",
    "**Key results from this run**\n",
    "- Alerts budget: **K = 5 assets/day**\n",
    "- Average assets selected per day: **5.00**\n",
    "- **Precision@K (asset-day) = 0.400**, meaning **~40% of the alerted asset-days** had at least one truly positive hour in the test labels.\n",
    "\n",
    "**Preview sanity check**\n",
    "- The “Top alerts” preview shows the expected daily Top-5 structure with:\n",
    "  `date_utc, asset_id, ts_peak, p_hat, y_asset_day` plus context (`site_id`, `line_id`, `asset_type`, `is_legacy`).\n",
    "- The daily rollup preview confirms day-level aggregation columns are populated and aligns with the same test window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338f5dbb-8eae-4142-8cab-802e7ab0d6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_alerts_top5_assets_per_day_test_drivers_long.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/panel_alerts_top5_assets_per_day_test_with_why.csv\n",
      "Updated manifest: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json\n",
      "\n",
      "Alerts + WHY (preview):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_utc</th>\n",
       "      <th>asset_id</th>\n",
       "      <th>ts_peak</th>\n",
       "      <th>p_hat</th>\n",
       "      <th>y_asset_day</th>\n",
       "      <th>row_ix</th>\n",
       "      <th>site_id</th>\n",
       "      <th>line_id</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>is_legacy</th>\n",
       "      <th>why_push_to_1</th>\n",
       "      <th>why_push_to_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>S2</td>\n",
       "      <td>S2-L4</td>\n",
       "      <td>case_packer</td>\n",
       "      <td>True</td>\n",
       "      <td>is_legacy(+0.631); asset_type_case_packer(+0.5...</td>\n",
       "      <td>dow_utc_cos(-0.508); site_id_S2(-0.212); dow_u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L4</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>True</td>\n",
       "      <td>is_legacy(+0.631); line_id_S4-L4(+0.588); is_w...</td>\n",
       "      <td>dow_utc_cos(-0.508); vibration_mm_s_tele_mean(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L1</td>\n",
       "      <td>cartoner</td>\n",
       "      <td>True</td>\n",
       "      <td>is_legacy(+0.631); humidity_rh_tele_mean(+0.47...</td>\n",
       "      <td>dow_utc_cos(-0.508); line_speed_u_min_tele_mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0063</td>\n",
       "      <td>2025-11-27 16:00:00+00:00</td>\n",
       "      <td>0.692411</td>\n",
       "      <td>0</td>\n",
       "      <td>4389</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L1</td>\n",
       "      <td>sterilizer</td>\n",
       "      <td>True</td>\n",
       "      <td>is_legacy(+0.631); humidity_rh_tele_max(+0.360...</td>\n",
       "      <td>dow_utc_cos(-0.508); humidity_rh_tele_mean(-0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0037</td>\n",
       "      <td>2025-11-27 22:00:00+00:00</td>\n",
       "      <td>0.682174</td>\n",
       "      <td>0</td>\n",
       "      <td>2378</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L3</td>\n",
       "      <td>capper</td>\n",
       "      <td>True</td>\n",
       "      <td>is_legacy(+0.631); asset_type_capper(+0.399); ...</td>\n",
       "      <td>dow_utc_cos(-0.508); site_id_S1(-0.332); humid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-28 11:00:00+00:00</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1</td>\n",
       "      <td>3400</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L4</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>True</td>\n",
       "      <td>inc_count(+10.214); is_legacy(+0.631); line_id...</td>\n",
       "      <td>humidity_rh_tele_mean(-0.810); dow_utc_cos(-0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-28 13:00:00+00:00</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1</td>\n",
       "      <td>4074</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L1</td>\n",
       "      <td>cartoner</td>\n",
       "      <td>True</td>\n",
       "      <td>inc_count(+10.214); is_legacy(+0.631); is_week...</td>\n",
       "      <td>dow_utc_cos(-0.508); site_id_S3(-0.301); temp_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0110</td>\n",
       "      <td>2025-11-28 04:00:00+00:00</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>1</td>\n",
       "      <td>7763</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L3</td>\n",
       "      <td>capper</td>\n",
       "      <td>True</td>\n",
       "      <td>inc_count(+10.214); is_legacy(+0.631); humidit...</td>\n",
       "      <td>dow_utc_cos(-0.508); vibration_mm_s_tele_mean(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0019</td>\n",
       "      <td>2025-11-28 05:00:00+00:00</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1</td>\n",
       "      <td>1376</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L5</td>\n",
       "      <td>labeler</td>\n",
       "      <td>True</td>\n",
       "      <td>inc_count(+10.214); is_legacy(+0.631); line_id...</td>\n",
       "      <td>dow_utc_cos(-0.508); asset_type_labeler(-0.409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>A0045</td>\n",
       "      <td>2025-11-28 14:00:00+00:00</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>1</td>\n",
       "      <td>3067</td>\n",
       "      <td>S2</td>\n",
       "      <td>S2-L2</td>\n",
       "      <td>labeler</td>\n",
       "      <td>True</td>\n",
       "      <td>inc_count(+10.214); is_legacy(+0.631); is_week...</td>\n",
       "      <td>dow_utc_cos(-0.508); asset_type_labeler(-0.409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0074</td>\n",
       "      <td>2025-11-29 08:00:00+00:00</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>1</td>\n",
       "      <td>5774</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L4</td>\n",
       "      <td>vision_inspection</td>\n",
       "      <td>True</td>\n",
       "      <td>inc_count(+10.214); is_legacy(+0.631); vibrati...</td>\n",
       "      <td>is_weekend_utc(-0.786); line_id_S3-L4(-0.512);...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0005</td>\n",
       "      <td>2025-11-29 03:00:00+00:00</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>1</td>\n",
       "      <td>388</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L2</td>\n",
       "      <td>environmental_monitor</td>\n",
       "      <td>True</td>\n",
       "      <td>inc_count(+10.214); is_legacy(+0.631); line_sp...</td>\n",
       "      <td>is_weekend_utc(-0.786); line_id_S4-L2(-0.733);...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-29 22:00:00+00:00</td>\n",
       "      <td>0.791923</td>\n",
       "      <td>1</td>\n",
       "      <td>3435</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L4</td>\n",
       "      <td>blister_packer</td>\n",
       "      <td>True</td>\n",
       "      <td>is_legacy(+0.631); line_id_S4-L4(+0.588); dow_...</td>\n",
       "      <td>is_weekend_utc(-0.786); dow_utc_cos(-0.125); h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-29 17:00:00+00:00</td>\n",
       "      <td>0.787405</td>\n",
       "      <td>0</td>\n",
       "      <td>7127</td>\n",
       "      <td>S2</td>\n",
       "      <td>S2-L4</td>\n",
       "      <td>case_packer</td>\n",
       "      <td>True</td>\n",
       "      <td>is_legacy(+0.631); asset_type_case_packer(+0.5...</td>\n",
       "      <td>is_weekend_utc(-0.786); site_id_S2(-0.212); do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>A0063</td>\n",
       "      <td>2025-11-29 23:00:00+00:00</td>\n",
       "      <td>0.705877</td>\n",
       "      <td>0</td>\n",
       "      <td>4444</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L1</td>\n",
       "      <td>sterilizer</td>\n",
       "      <td>True</td>\n",
       "      <td>is_legacy(+0.631); humidity_rh_tele_max(+0.517...</td>\n",
       "      <td>humidity_rh_tele_mean(-0.792); is_weekend_utc(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_utc asset_id                   ts_peak     p_hat  y_asset_day  \\\n",
       "0   2025-11-27    A0105 2025-11-27 20:00:00+00:00  0.753193            0   \n",
       "1   2025-11-27    A0046 2025-11-27 18:00:00+00:00  0.731878            0   \n",
       "2   2025-11-27    A0056 2025-11-27 19:00:00+00:00  0.693739            0   \n",
       "3   2025-11-27    A0063 2025-11-27 16:00:00+00:00  0.692411            0   \n",
       "4   2025-11-27    A0037 2025-11-27 22:00:00+00:00  0.682174            0   \n",
       "5   2025-11-28    A0046 2025-11-28 11:00:00+00:00  0.999991            1   \n",
       "6   2025-11-28    A0056 2025-11-28 13:00:00+00:00  0.999987            1   \n",
       "7   2025-11-28    A0110 2025-11-28 04:00:00+00:00  0.999983            1   \n",
       "8   2025-11-28    A0019 2025-11-28 05:00:00+00:00  0.999980            1   \n",
       "9   2025-11-28    A0045 2025-11-28 14:00:00+00:00  0.999951            1   \n",
       "10  2025-11-29    A0074 2025-11-29 08:00:00+00:00  0.999956            1   \n",
       "11  2025-11-29    A0005 2025-11-29 03:00:00+00:00  0.999943            1   \n",
       "12  2025-11-29    A0046 2025-11-29 22:00:00+00:00  0.791923            1   \n",
       "13  2025-11-29    A0105 2025-11-29 17:00:00+00:00  0.787405            0   \n",
       "14  2025-11-29    A0063 2025-11-29 23:00:00+00:00  0.705877            0   \n",
       "\n",
       "    row_ix site_id line_id             asset_type  is_legacy  \\\n",
       "0     7082      S2   S2-L4            case_packer       True   \n",
       "1     3383      S4   S4-L4         blister_packer       True   \n",
       "2     4056      S3   S3-L1               cartoner       True   \n",
       "3     4389      S3   S3-L1             sterilizer       True   \n",
       "4     2378      S1   S1-L3                 capper       True   \n",
       "5     3400      S4   S4-L4         blister_packer       True   \n",
       "6     4074      S3   S3-L1               cartoner       True   \n",
       "7     7763      S4   S4-L3                 capper       True   \n",
       "8     1376      S3   S3-L5                labeler       True   \n",
       "9     3067      S2   S2-L2                labeler       True   \n",
       "10    5774      S3   S3-L4      vision_inspection       True   \n",
       "11     388      S4   S4-L2  environmental_monitor       True   \n",
       "12    3435      S4   S4-L4         blister_packer       True   \n",
       "13    7127      S2   S2-L4            case_packer       True   \n",
       "14    4444      S3   S3-L1             sterilizer       True   \n",
       "\n",
       "                                        why_push_to_1  \\\n",
       "0   is_legacy(+0.631); asset_type_case_packer(+0.5...   \n",
       "1   is_legacy(+0.631); line_id_S4-L4(+0.588); is_w...   \n",
       "2   is_legacy(+0.631); humidity_rh_tele_mean(+0.47...   \n",
       "3   is_legacy(+0.631); humidity_rh_tele_max(+0.360...   \n",
       "4   is_legacy(+0.631); asset_type_capper(+0.399); ...   \n",
       "5   inc_count(+10.214); is_legacy(+0.631); line_id...   \n",
       "6   inc_count(+10.214); is_legacy(+0.631); is_week...   \n",
       "7   inc_count(+10.214); is_legacy(+0.631); humidit...   \n",
       "8   inc_count(+10.214); is_legacy(+0.631); line_id...   \n",
       "9   inc_count(+10.214); is_legacy(+0.631); is_week...   \n",
       "10  inc_count(+10.214); is_legacy(+0.631); vibrati...   \n",
       "11  inc_count(+10.214); is_legacy(+0.631); line_sp...   \n",
       "12  is_legacy(+0.631); line_id_S4-L4(+0.588); dow_...   \n",
       "13  is_legacy(+0.631); asset_type_case_packer(+0.5...   \n",
       "14  is_legacy(+0.631); humidity_rh_tele_max(+0.517...   \n",
       "\n",
       "                                        why_push_to_0  \n",
       "0   dow_utc_cos(-0.508); site_id_S2(-0.212); dow_u...  \n",
       "1   dow_utc_cos(-0.508); vibration_mm_s_tele_mean(...  \n",
       "2   dow_utc_cos(-0.508); line_speed_u_min_tele_mea...  \n",
       "3   dow_utc_cos(-0.508); humidity_rh_tele_mean(-0....  \n",
       "4   dow_utc_cos(-0.508); site_id_S1(-0.332); humid...  \n",
       "5   humidity_rh_tele_mean(-0.810); dow_utc_cos(-0....  \n",
       "6   dow_utc_cos(-0.508); site_id_S3(-0.301); temp_...  \n",
       "7   dow_utc_cos(-0.508); vibration_mm_s_tele_mean(...  \n",
       "8   dow_utc_cos(-0.508); asset_type_labeler(-0.409...  \n",
       "9   dow_utc_cos(-0.508); asset_type_labeler(-0.409...  \n",
       "10  is_weekend_utc(-0.786); line_id_S3-L4(-0.512);...  \n",
       "11  is_weekend_utc(-0.786); line_id_S4-L2(-0.733);...  \n",
       "12  is_weekend_utc(-0.786); dow_utc_cos(-0.125); h...  \n",
       "13  is_weekend_utc(-0.786); site_id_S2(-0.212); do...  \n",
       "14  humidity_rh_tele_mean(-0.792); is_weekend_utc(...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Drivers (long form, preview):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_utc</th>\n",
       "      <th>asset_id</th>\n",
       "      <th>ts_peak</th>\n",
       "      <th>p_hat_peak</th>\n",
       "      <th>y_asset_day</th>\n",
       "      <th>row_ix</th>\n",
       "      <th>feature</th>\n",
       "      <th>x</th>\n",
       "      <th>coef</th>\n",
       "      <th>contrib</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>is_legacy</td>\n",
       "      <td>0.960621</td>\n",
       "      <td>0.656359</td>\n",
       "      <td>0.630512</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>asset_type_case_packer</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.531498</td>\n",
       "      <td>0.531498</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>dow_utc_cos</td>\n",
       "      <td>-1.271777</td>\n",
       "      <td>0.399179</td>\n",
       "      <td>-0.507667</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>line_id_S2-L4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363836</td>\n",
       "      <td>0.363836</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>is_weekend_utc</td>\n",
       "      <td>-0.631791</td>\n",
       "      <td>-0.496756</td>\n",
       "      <td>0.313846</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>site_id_S2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.212079</td>\n",
       "      <td>-0.212079</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>dow_utc_sin</td>\n",
       "      <td>0.612969</td>\n",
       "      <td>-0.267724</td>\n",
       "      <td>-0.164107</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>telemetry_rows_hour</td>\n",
       "      <td>-1.202494</td>\n",
       "      <td>-0.110827</td>\n",
       "      <td>0.133269</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>humidity_rh_tele_max</td>\n",
       "      <td>-0.655394</td>\n",
       "      <td>-0.171087</td>\n",
       "      <td>0.112130</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0105</td>\n",
       "      <td>2025-11-27 20:00:00+00:00</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0</td>\n",
       "      <td>7082</td>\n",
       "      <td>humidity_rh_tele_min</td>\n",
       "      <td>0.584919</td>\n",
       "      <td>-0.138792</td>\n",
       "      <td>-0.081182</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>is_legacy</td>\n",
       "      <td>0.960621</td>\n",
       "      <td>0.656359</td>\n",
       "      <td>0.630512</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>line_id_S4-L4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588401</td>\n",
       "      <td>0.588401</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>dow_utc_cos</td>\n",
       "      <td>-1.271777</td>\n",
       "      <td>0.399179</td>\n",
       "      <td>-0.507667</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>is_weekend_utc</td>\n",
       "      <td>-0.631791</td>\n",
       "      <td>-0.496756</td>\n",
       "      <td>0.313846</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>vibration_mm_s_tele_mean</td>\n",
       "      <td>-1.779063</td>\n",
       "      <td>0.173254</td>\n",
       "      <td>-0.308231</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>line_speed_u_min_tele_mean</td>\n",
       "      <td>1.274537</td>\n",
       "      <td>0.169922</td>\n",
       "      <td>0.216571</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>dow_utc_sin</td>\n",
       "      <td>0.612969</td>\n",
       "      <td>-0.267724</td>\n",
       "      <td>-0.164107</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>line_speed_u_min_tele_min</td>\n",
       "      <td>1.719643</td>\n",
       "      <td>-0.092673</td>\n",
       "      <td>-0.159365</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>vibration_mm_s_tele_min</td>\n",
       "      <td>-0.899103</td>\n",
       "      <td>-0.171638</td>\n",
       "      <td>0.154320</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0046</td>\n",
       "      <td>2025-11-27 18:00:00+00:00</td>\n",
       "      <td>0.731878</td>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>humidity_rh_tele_max</td>\n",
       "      <td>-0.874215</td>\n",
       "      <td>-0.171087</td>\n",
       "      <td>0.149567</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>is_legacy</td>\n",
       "      <td>0.960621</td>\n",
       "      <td>0.656359</td>\n",
       "      <td>0.630512</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>dow_utc_cos</td>\n",
       "      <td>-1.271777</td>\n",
       "      <td>0.399179</td>\n",
       "      <td>-0.507667</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>humidity_rh_tele_mean</td>\n",
       "      <td>1.712470</td>\n",
       "      <td>0.278731</td>\n",
       "      <td>0.477318</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>line_speed_u_min_tele_mean</td>\n",
       "      <td>-2.655351</td>\n",
       "      <td>0.169922</td>\n",
       "      <td>-0.451201</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>humidity_rh_tele_max</td>\n",
       "      <td>2.133749</td>\n",
       "      <td>-0.171087</td>\n",
       "      <td>-0.365058</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>is_weekend_utc</td>\n",
       "      <td>-0.631791</td>\n",
       "      <td>-0.496756</td>\n",
       "      <td>0.313846</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>line_id_S3-L1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.311943</td>\n",
       "      <td>0.311943</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>site_id_S3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.301494</td>\n",
       "      <td>-0.301494</td>\n",
       "      <td>push_to_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>line_speed_u_min_tele_max</td>\n",
       "      <td>-2.849033</td>\n",
       "      <td>-0.097975</td>\n",
       "      <td>0.279134</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>A0056</td>\n",
       "      <td>2025-11-27 19:00:00+00:00</td>\n",
       "      <td>0.693739</td>\n",
       "      <td>0</td>\n",
       "      <td>4056</td>\n",
       "      <td>asset_type_cartoner</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266095</td>\n",
       "      <td>0.266095</td>\n",
       "      <td>push_to_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_utc asset_id                   ts_peak  p_hat_peak  y_asset_day  \\\n",
       "0   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "1   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "2   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "3   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "4   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "5   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "6   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "7   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "8   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "9   2025-11-27    A0105 2025-11-27 20:00:00+00:00    0.753193            0   \n",
       "10  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "11  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "12  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "13  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "14  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "15  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "16  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "17  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "18  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "19  2025-11-27    A0046 2025-11-27 18:00:00+00:00    0.731878            0   \n",
       "20  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "21  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "22  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "23  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "24  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "25  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "26  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "27  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "28  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "29  2025-11-27    A0056 2025-11-27 19:00:00+00:00    0.693739            0   \n",
       "\n",
       "    row_ix                     feature         x      coef   contrib  \\\n",
       "0     7082                   is_legacy  0.960621  0.656359  0.630512   \n",
       "1     7082      asset_type_case_packer  1.000000  0.531498  0.531498   \n",
       "2     7082                 dow_utc_cos -1.271777  0.399179 -0.507667   \n",
       "3     7082               line_id_S2-L4  1.000000  0.363836  0.363836   \n",
       "4     7082              is_weekend_utc -0.631791 -0.496756  0.313846   \n",
       "5     7082                  site_id_S2  1.000000 -0.212079 -0.212079   \n",
       "6     7082                 dow_utc_sin  0.612969 -0.267724 -0.164107   \n",
       "7     7082         telemetry_rows_hour -1.202494 -0.110827  0.133269   \n",
       "8     7082        humidity_rh_tele_max -0.655394 -0.171087  0.112130   \n",
       "9     7082        humidity_rh_tele_min  0.584919 -0.138792 -0.081182   \n",
       "10    3383                   is_legacy  0.960621  0.656359  0.630512   \n",
       "11    3383               line_id_S4-L4  1.000000  0.588401  0.588401   \n",
       "12    3383                 dow_utc_cos -1.271777  0.399179 -0.507667   \n",
       "13    3383              is_weekend_utc -0.631791 -0.496756  0.313846   \n",
       "14    3383    vibration_mm_s_tele_mean -1.779063  0.173254 -0.308231   \n",
       "15    3383  line_speed_u_min_tele_mean  1.274537  0.169922  0.216571   \n",
       "16    3383                 dow_utc_sin  0.612969 -0.267724 -0.164107   \n",
       "17    3383   line_speed_u_min_tele_min  1.719643 -0.092673 -0.159365   \n",
       "18    3383     vibration_mm_s_tele_min -0.899103 -0.171638  0.154320   \n",
       "19    3383        humidity_rh_tele_max -0.874215 -0.171087  0.149567   \n",
       "20    4056                   is_legacy  0.960621  0.656359  0.630512   \n",
       "21    4056                 dow_utc_cos -1.271777  0.399179 -0.507667   \n",
       "22    4056       humidity_rh_tele_mean  1.712470  0.278731  0.477318   \n",
       "23    4056  line_speed_u_min_tele_mean -2.655351  0.169922 -0.451201   \n",
       "24    4056        humidity_rh_tele_max  2.133749 -0.171087 -0.365058   \n",
       "25    4056              is_weekend_utc -0.631791 -0.496756  0.313846   \n",
       "26    4056               line_id_S3-L1  1.000000  0.311943  0.311943   \n",
       "27    4056                  site_id_S3  1.000000 -0.301494 -0.301494   \n",
       "28    4056   line_speed_u_min_tele_max -2.849033 -0.097975  0.279134   \n",
       "29    4056         asset_type_cartoner  1.000000  0.266095  0.266095   \n",
       "\n",
       "    direction  \n",
       "0   push_to_1  \n",
       "1   push_to_1  \n",
       "2   push_to_0  \n",
       "3   push_to_1  \n",
       "4   push_to_1  \n",
       "5   push_to_0  \n",
       "6   push_to_0  \n",
       "7   push_to_1  \n",
       "8   push_to_1  \n",
       "9   push_to_0  \n",
       "10  push_to_1  \n",
       "11  push_to_1  \n",
       "12  push_to_0  \n",
       "13  push_to_1  \n",
       "14  push_to_0  \n",
       "15  push_to_1  \n",
       "16  push_to_0  \n",
       "17  push_to_0  \n",
       "18  push_to_1  \n",
       "19  push_to_1  \n",
       "20  push_to_1  \n",
       "21  push_to_0  \n",
       "22  push_to_1  \n",
       "23  push_to_0  \n",
       "24  push_to_0  \n",
       "25  push_to_1  \n",
       "26  push_to_1  \n",
       "27  push_to_0  \n",
       "28  push_to_1  \n",
       "29  push_to_1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 4 — Alert “WHY” layer: top drivers per alerted asset-day (TEST)\n",
    "#   Uses the Top-5 assets/day alerts from Cell 3 and explains each alert\n",
    "#   by extracting top coefficient contributions from the sparse design row\n",
    "#   Robust to DASHBOARD_EXPORTS.json schema (exports may be a list OR dict)\n",
    "#============================================================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Locate required inputs (from prior cells)\n",
    "# -----------------------------\n",
    "# Expect these to exist from Cell 1:\n",
    "#   PANEL_RUN_DIR, EXPORT_DIR\n",
    "alerts_path = EXPORT_DIR / \"panel_alerts_top5_assets_per_day_test.csv\"\n",
    "X_test_path = PANEL_RUN_DIR / \"panel_X_test.npz\"\n",
    "feat_path   = PANEL_RUN_DIR / \"panel_feature_names.csv\"\n",
    "model_path  = PANEL_RUN_DIR / \"panel_baseline_logreg_saga.joblib\"\n",
    "\n",
    "for p in [alerts_path, X_test_path, feat_path, model_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "alerts = pd.read_csv(alerts_path)\n",
    "X_test_tx = sp.load_npz(X_test_path)\n",
    "clf = joblib.load(model_path)\n",
    "\n",
    "# Force CSR for fast row slicing\n",
    "if not sp.issparse(X_test_tx):\n",
    "    X_test_tx = sp.csr_matrix(X_test_tx)\n",
    "else:\n",
    "    X_test_tx = X_test_tx.tocsr()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load feature names robustly (column schema may vary)\n",
    "# -----------------------------\n",
    "feat_df = pd.read_csv(feat_path)\n",
    "\n",
    "if \"feature_name\" in feat_df.columns:\n",
    "    feature_names = feat_df[\"feature_name\"].astype(str).tolist()\n",
    "elif \"feature\" in feat_df.columns:\n",
    "    feature_names = feat_df[\"feature\"].astype(str).tolist()\n",
    "elif len(feat_df.columns) == 1:\n",
    "    feature_names = feat_df.iloc[:, 0].astype(str).tolist()\n",
    "else:\n",
    "    feature_names = feat_df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "# Sanity check: features align with matrix columns\n",
    "if X_test_tx.shape[1] != len(feature_names):\n",
    "    raise ValueError(\n",
    "        \"Feature dimension mismatch:\\n\"\n",
    "        f\"  X_test_tx.shape[1] = {X_test_tx.shape[1]}\\n\"\n",
    "        f\"  len(feature_names) = {len(feature_names)}\\n\"\n",
    "        f\"  feature_names file = {feat_path}\"\n",
    "    )\n",
    "\n",
    "# Model coefficient vector\n",
    "coefs = clf.coef_.ravel()\n",
    "if len(coefs) != X_test_tx.shape[1]:\n",
    "    raise ValueError(\n",
    "        \"Model coefficient dimension mismatch:\\n\"\n",
    "        f\"  len(coefs)          = {len(coefs)}\\n\"\n",
    "        f\"  X_test_tx.shape[1]  = {X_test_tx.shape[1]}\\n\"\n",
    "        f\"  model file          = {model_path}\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Ensure required columns exist in alerts\n",
    "# -----------------------------\n",
    "required_cols = [\"date_utc\", \"asset_id\", \"ts_peak\", \"p_hat\", \"y_asset_day\", \"row_ix\"]\n",
    "missing = [c for c in required_cols if c not in alerts.columns]\n",
    "if missing:\n",
    "    raise KeyError(\n",
    "        \"Alerts file is missing required columns.\\n\"\n",
    "        f\"  Missing: {missing}\\n\"\n",
    "        f\"  Found: {alerts.columns.tolist()}\\n\"\n",
    "        f\"  File: {alerts_path}\"\n",
    "    )\n",
    "\n",
    "alerts[\"ts_peak\"] = pd.to_datetime(alerts[\"ts_peak\"], utc=True, errors=\"coerce\")\n",
    "if alerts[\"ts_peak\"].isna().any():\n",
    "    raise ValueError(\"alerts['ts_peak'] contains NaT after parsing; check the alerts export.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Per-alert driver extraction (top contributions)\n",
    "#    contribution_i = x_i * coef_i  (in transformed feature space)\n",
    "# -----------------------------\n",
    "def top_contribs_for_row(row_ix: int, top_n: int = 10) -> pd.DataFrame:\n",
    "    row = X_test_tx.getrow(int(row_ix))  # 1 x n_features CSR\n",
    "    if row.nnz == 0:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"x\", \"coef\", \"contrib\", \"direction\"])\n",
    "\n",
    "    idx = row.indices\n",
    "    x = row.data\n",
    "    c = coefs[idx]\n",
    "    contrib = x * c\n",
    "\n",
    "    dfc = pd.DataFrame({\n",
    "        \"feature\": [feature_names[i] for i in idx],\n",
    "        \"x\": x,\n",
    "        \"coef\": c,\n",
    "        \"contrib\": contrib,\n",
    "    })\n",
    "    dfc[\"abs_contrib\"] = np.abs(dfc[\"contrib\"])\n",
    "    dfc[\"direction\"] = np.where(dfc[\"contrib\"] >= 0, \"push_to_1\", \"push_to_0\")\n",
    "\n",
    "    return dfc.sort_values(\"abs_contrib\", ascending=False).drop(columns=[\"abs_contrib\"]).head(top_n)\n",
    "\n",
    "TOP_N = 10\n",
    "drivers_long = []\n",
    "\n",
    "for _, r in alerts.iterrows():\n",
    "    row_ix = int(r[\"row_ix\"])\n",
    "    d = top_contribs_for_row(row_ix=row_ix, top_n=TOP_N).copy()\n",
    "\n",
    "    d.insert(0, \"date_utc\", r[\"date_utc\"])\n",
    "    d.insert(1, \"asset_id\", r[\"asset_id\"])\n",
    "    d.insert(2, \"ts_peak\", r[\"ts_peak\"])\n",
    "    d.insert(3, \"p_hat_peak\", float(r[\"p_hat\"]))\n",
    "    d.insert(4, \"y_asset_day\", int(r[\"y_asset_day\"]))\n",
    "    d.insert(5, \"row_ix\", row_ix)\n",
    "\n",
    "    drivers_long.append(d)\n",
    "\n",
    "drivers_long_df = pd.concat(drivers_long, ignore_index=True) if drivers_long else pd.DataFrame()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Compact “why” strings per alert row (dashboard friendly)\n",
    "# -----------------------------\n",
    "def _summarize_why(dsub: pd.DataFrame, n_each: int = 3) -> dict:\n",
    "    if dsub.empty:\n",
    "        return {\"why_push_to_1\": \"\", \"why_push_to_0\": \"\"}\n",
    "\n",
    "    pos = dsub[dsub[\"contrib\"] > 0].copy().sort_values(\"contrib\", ascending=False).head(n_each)\n",
    "    neg = dsub[dsub[\"contrib\"] < 0].copy().sort_values(\"contrib\", ascending=True).head(n_each)\n",
    "\n",
    "    def fmt(df):\n",
    "        return \"; \".join([f\"{f}({v:+.3f})\" for f, v in zip(df[\"feature\"], df[\"contrib\"])])\n",
    "\n",
    "    return {\"why_push_to_1\": fmt(pos), \"why_push_to_0\": fmt(neg)}\n",
    "\n",
    "why_rows = []\n",
    "if not drivers_long_df.empty:\n",
    "    for (date_utc, asset_id, ts_peak, row_ix), dsub in drivers_long_df.groupby(\n",
    "        [\"date_utc\", \"asset_id\", \"ts_peak\", \"row_ix\"], as_index=False\n",
    "    ):\n",
    "        s = _summarize_why(dsub, n_each=3)\n",
    "        why_rows.append({\n",
    "            \"date_utc\": date_utc,\n",
    "            \"asset_id\": asset_id,\n",
    "            \"ts_peak\": ts_peak,\n",
    "            \"row_ix\": row_ix,\n",
    "            **s\n",
    "        })\n",
    "\n",
    "why_df = pd.DataFrame(why_rows)\n",
    "alerts_why = alerts.merge(why_df, on=[\"date_utc\", \"asset_id\", \"ts_peak\", \"row_ix\"], how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Persist outputs + update manifest (robust to exports being list or dict)\n",
    "# -----------------------------\n",
    "drivers_long_out = EXPORT_DIR / \"panel_alerts_top5_assets_per_day_test_drivers_long.csv\"\n",
    "alerts_why_out   = EXPORT_DIR / \"panel_alerts_top5_assets_per_day_test_with_why.csv\"\n",
    "\n",
    "drivers_long_df.to_csv(drivers_long_out, index=False)\n",
    "alerts_why.to_csv(alerts_why_out, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", drivers_long_out)\n",
    "print(\" \", alerts_why_out)\n",
    "\n",
    "def _upsert_export(manifest_obj: dict, key: str, path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Supports either:\n",
    "      - manifest[\"exports\"] as dict  -> set exports[key] = path\n",
    "      - manifest[\"exports\"] as list  -> append/replace {\"name\": key, \"path\": path}\n",
    "    \"\"\"\n",
    "    exports = manifest_obj.get(\"exports\", None)\n",
    "\n",
    "    # If missing, default to dict (simple, stable)\n",
    "    if exports is None:\n",
    "        manifest_obj[\"exports\"] = {key: path}\n",
    "        return manifest_obj\n",
    "\n",
    "    # If dict, write directly\n",
    "    if isinstance(exports, dict):\n",
    "        exports[key] = path\n",
    "        manifest_obj[\"exports\"] = exports\n",
    "        return manifest_obj\n",
    "\n",
    "    # If list, store as list-of-records\n",
    "    if isinstance(exports, list):\n",
    "        replaced = False\n",
    "        new_list = []\n",
    "        for item in exports:\n",
    "            if isinstance(item, dict) and item.get(\"name\") == key:\n",
    "                new_list.append({\"name\": key, \"path\": path})\n",
    "                replaced = True\n",
    "            else:\n",
    "                new_list.append(item)\n",
    "        if not replaced:\n",
    "            new_list.append({\"name\": key, \"path\": path})\n",
    "        manifest_obj[\"exports\"] = new_list\n",
    "        return manifest_obj\n",
    "\n",
    "    # Otherwise (unexpected type), coerce to dict\n",
    "    manifest_obj[\"exports\"] = {key: path}\n",
    "    return manifest_obj\n",
    "\n",
    "manifest_path = EXPORT_DIR / \"DASHBOARD_EXPORTS.json\"\n",
    "manifest = json.loads(manifest_path.read_text()) if manifest_path.exists() else {}\n",
    "\n",
    "manifest = _upsert_export(manifest, \"panel_alerts_top5_assets_per_day_test_drivers_long_csv\", str(drivers_long_out))\n",
    "manifest = _upsert_export(manifest, \"panel_alerts_top5_assets_per_day_test_with_why_csv\", str(alerts_why_out))\n",
    "\n",
    "manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "print(\"Updated manifest:\", manifest_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Quick preview\n",
    "# -----------------------------\n",
    "print(\"\\nAlerts + WHY (preview):\")\n",
    "display(alerts_why.head(15))\n",
    "\n",
    "print(\"\\nDrivers (long form, preview):\")\n",
    "display(drivers_long_df.head(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83598cfe-62a2-4383-ba29-2f7291dc7571",
   "metadata": {},
   "source": [
    "### What Cell 4 Just Did — “Why” Layer for Top-5 Alerts/Day (Explainability)\n",
    "\n",
    "This cell adds an **actionability layer** on top of the **Top-5 assets/day alert budget** produced in Cell 3 by generating a human-readable *why* for each alert.\n",
    "\n",
    "**Inputs used**\n",
    "- The daily alert list from the scoring run: `panel_alerts_top5_assets_per_day_test.csv`\n",
    "- The corresponding TEST design matrix and model artifacts from the selected panel run:\n",
    "  - `panel_X_test.npz` (sparse transformed features)\n",
    "  - `panel_feature_names.csv` (feature name mapping)\n",
    "  - `panel_baseline_logreg_saga.joblib` (trained logistic regression)\n",
    "\n",
    "**What it computed**\n",
    "- For every alerted **asset-day**, it took the **peak-risk hour** (`ts_peak`) and used the stored `row_ix` (which points directly into the sparse `X_test` row) to compute **feature contribution values** in the transformed feature space:\n",
    "\n",
    "  \\[\n",
    "  \\text{contrib}_i = x_i \\times \\beta_i\n",
    "  \\]\n",
    "\n",
    "  Where:\n",
    "  - \\(x_i\\) is the transformed feature value for that hour\n",
    "  - \\(\\beta_i\\) is the model coefficient for that feature\n",
    "\n",
    "- It then produced two explainability views:\n",
    "  1. **Long-form drivers**: top per-row contributions with sign/direction (`push_to_1` vs `push_to_0`)\n",
    "  2. **Compact “why” strings** per alert row:\n",
    "     - `why_push_to_1`: the strongest contributors pushing risk upward\n",
    "     - `why_push_to_0`: the strongest contributors pushing risk downward\n",
    "\n",
    "**What the preview shows**\n",
    "- The alert table now includes “why” text. For example:\n",
    "  - Many high-risk alerts include `inc_count(+10.214)` as a dominant push-to-1 driver, which aligns with the model learning that recent incident frequency strongly increases predicted risk.\n",
    "  - Some alerts are also influenced by static/context features like `is_legacy(+0.631)` or line/site indicators (e.g., `line_id_S4-L4(+0.588)`), while time features like `dow_utc_cos(-0.508)` can push predictions down depending on the hour/day position.\n",
    "\n",
    "**Files created**\n",
    "- `panel_alerts_top5_assets_per_day_test_drivers_long.csv`  \n",
    "  Long-form driver table: one row per `(alert × feature)` with `x`, `coef`, `contrib`, and direction.\n",
    "- `panel_alerts_top5_assets_per_day_test_with_why.csv`  \n",
    "  Business-friendly alert table with compact `why_push_to_1` / `why_push_to_0` summaries.\n",
    "\n",
    "**Provenance**\n",
    "- `DASHBOARD_EXPORTS.json` was updated successfully to register these new export artifacts, and the update is robust to the manifest’s `exports` structure (list or dict).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8bc43a-881e-4fe0-9a33-c2669bdb72e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest day (UTC): 2025-12-11\n",
      "\n",
      "Saved dashboard bundle:\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/dashboard_alert_queue_latest_day_test.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/dashboard_top_sites_latest_day_test.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/dashboard_top_lines_latest_day_test.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/dashboard_daily_trend_last14_test.csv\n",
      "  /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/dashboard_kpis_test.json\n",
      "\n",
      "Updated manifest: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json\n",
      "\n",
      "Alert queue (latest day) preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_utc</th>\n",
       "      <th>asset_id</th>\n",
       "      <th>ts_peak</th>\n",
       "      <th>p_hat</th>\n",
       "      <th>y_asset_day</th>\n",
       "      <th>site_id</th>\n",
       "      <th>line_id</th>\n",
       "      <th>asset_type</th>\n",
       "      <th>is_legacy</th>\n",
       "      <th>why_push_to_1</th>\n",
       "      <th>why_push_to_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>A0011</td>\n",
       "      <td>2025-12-11 00:00:00+00:00</td>\n",
       "      <td>0.297370</td>\n",
       "      <td>0</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L3</td>\n",
       "      <td>vision_inspection</td>\n",
       "      <td>False</td>\n",
       "      <td>is_weekend_utc(+0.314); asset_type_vision_insp...</td>\n",
       "      <td>is_legacy(-0.683); dow_utc_cos(-0.508); site_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>A0032</td>\n",
       "      <td>2025-12-11 00:00:00+00:00</td>\n",
       "      <td>0.275497</td>\n",
       "      <td>0</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L3</td>\n",
       "      <td>vision_inspection</td>\n",
       "      <td>False</td>\n",
       "      <td>line_speed_u_min_tele_mean(+0.452); is_weekend...</td>\n",
       "      <td>is_legacy(-0.683); dow_utc_cos(-0.508); line_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>A0065</td>\n",
       "      <td>2025-12-11 00:00:00+00:00</td>\n",
       "      <td>0.251767</td>\n",
       "      <td>0</td>\n",
       "      <td>S3</td>\n",
       "      <td>S3-L5</td>\n",
       "      <td>labeler</td>\n",
       "      <td>False</td>\n",
       "      <td>line_id_S3-L5(+0.553); is_weekend_utc(+0.314);...</td>\n",
       "      <td>is_legacy(-0.683); dow_utc_cos(-0.508); asset_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>A0108</td>\n",
       "      <td>2025-12-11 00:00:00+00:00</td>\n",
       "      <td>0.250631</td>\n",
       "      <td>0</td>\n",
       "      <td>S1</td>\n",
       "      <td>S1-L4</td>\n",
       "      <td>cartoner</td>\n",
       "      <td>False</td>\n",
       "      <td>is_weekend_utc(+0.314); asset_type_cartoner(+0...</td>\n",
       "      <td>is_legacy(-0.683); dow_utc_cos(-0.508); site_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>A0012</td>\n",
       "      <td>2025-12-11 00:00:00+00:00</td>\n",
       "      <td>0.163610</td>\n",
       "      <td>0</td>\n",
       "      <td>S4</td>\n",
       "      <td>S4-L3</td>\n",
       "      <td>weigh_check</td>\n",
       "      <td>False</td>\n",
       "      <td>is_weekend_utc(+0.314); telemetry_rows_hour(+0...</td>\n",
       "      <td>is_legacy(-0.683); dow_utc_cos(-0.508); asset_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_utc asset_id                   ts_peak     p_hat  y_asset_day  \\\n",
       "0  2025-12-11    A0011 2025-12-11 00:00:00+00:00  0.297370            0   \n",
       "1  2025-12-11    A0032 2025-12-11 00:00:00+00:00  0.275497            0   \n",
       "2  2025-12-11    A0065 2025-12-11 00:00:00+00:00  0.251767            0   \n",
       "3  2025-12-11    A0108 2025-12-11 00:00:00+00:00  0.250631            0   \n",
       "4  2025-12-11    A0012 2025-12-11 00:00:00+00:00  0.163610            0   \n",
       "\n",
       "  site_id line_id         asset_type  is_legacy  \\\n",
       "0      S1   S1-L3  vision_inspection      False   \n",
       "1      S4   S4-L3  vision_inspection      False   \n",
       "2      S3   S3-L5            labeler      False   \n",
       "3      S1   S1-L4           cartoner      False   \n",
       "4      S4   S4-L3        weigh_check      False   \n",
       "\n",
       "                                       why_push_to_1  \\\n",
       "0  is_weekend_utc(+0.314); asset_type_vision_insp...   \n",
       "1  line_speed_u_min_tele_mean(+0.452); is_weekend...   \n",
       "2  line_id_S3-L5(+0.553); is_weekend_utc(+0.314);...   \n",
       "3  is_weekend_utc(+0.314); asset_type_cartoner(+0...   \n",
       "4  is_weekend_utc(+0.314); telemetry_rows_hour(+0...   \n",
       "\n",
       "                                       why_push_to_0  \n",
       "0  is_legacy(-0.683); dow_utc_cos(-0.508); site_i...  \n",
       "1  is_legacy(-0.683); dow_utc_cos(-0.508); line_i...  \n",
       "2  is_legacy(-0.683); dow_utc_cos(-0.508); asset_...  \n",
       "3  is_legacy(-0.683); dow_utc_cos(-0.508); site_i...  \n",
       "4  is_legacy(-0.683); dow_utc_cos(-0.508); asset_...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top sites (latest day) preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_utc</th>\n",
       "      <th>site_id</th>\n",
       "      <th>n_hours</th>\n",
       "      <th>n_pos_hours</th>\n",
       "      <th>pos_rate_hours</th>\n",
       "      <th>p_hat_mean</th>\n",
       "      <th>p_hat_max</th>\n",
       "      <th>n_assets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>S1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190104</td>\n",
       "      <td>0.297370</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>S4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219553</td>\n",
       "      <td>0.275497</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.251767</td>\n",
       "      <td>0.251767</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_utc site_id  n_hours  n_pos_hours  pos_rate_hours  p_hat_mean  \\\n",
       "0  2025-12-11      S1        4            0             0.0    0.190104   \n",
       "1  2025-12-11      S4        2            0             0.0    0.219553   \n",
       "2  2025-12-11      S3        1            0             0.0    0.251767   \n",
       "\n",
       "   p_hat_max  n_assets  \n",
       "0   0.297370         4  \n",
       "1   0.275497         2  \n",
       "2   0.251767         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top lines (latest day) preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_utc</th>\n",
       "      <th>line_id</th>\n",
       "      <th>n_hours</th>\n",
       "      <th>n_pos_hours</th>\n",
       "      <th>pos_rate_hours</th>\n",
       "      <th>p_hat_mean</th>\n",
       "      <th>p_hat_max</th>\n",
       "      <th>n_assets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>S1-L3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297370</td>\n",
       "      <td>0.297370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>S4-L3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219553</td>\n",
       "      <td>0.275497</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>S3-L5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.251767</td>\n",
       "      <td>0.251767</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>S1-L4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250631</td>\n",
       "      <td>0.250631</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>S1-L2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106208</td>\n",
       "      <td>0.132054</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_utc line_id  n_hours  n_pos_hours  pos_rate_hours  p_hat_mean  \\\n",
       "0  2025-12-11   S1-L3        1            0             0.0    0.297370   \n",
       "1  2025-12-11   S4-L3        2            0             0.0    0.219553   \n",
       "2  2025-12-11   S3-L5        1            0             0.0    0.251767   \n",
       "3  2025-12-11   S1-L4        1            0             0.0    0.250631   \n",
       "4  2025-12-11   S1-L2        2            0             0.0    0.106208   \n",
       "\n",
       "   p_hat_max  n_assets  \n",
       "0   0.297370         1  \n",
       "1   0.275497         2  \n",
       "2   0.251767         1  \n",
       "3   0.250631         1  \n",
       "4   0.132054         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daily trend (last 14 days) preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_utc</th>\n",
       "      <th>n_hours</th>\n",
       "      <th>n_pos_hours</th>\n",
       "      <th>pos_rate_hours</th>\n",
       "      <th>p_hat_mean</th>\n",
       "      <th>p_hat_p95</th>\n",
       "      <th>p_hat_max</th>\n",
       "      <th>n_assets</th>\n",
       "      <th>n_alert_assets</th>\n",
       "      <th>n_alert_asset_days_positive</th>\n",
       "      <th>precision_at_k</th>\n",
       "      <th>max_alert_p_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-28</td>\n",
       "      <td>576</td>\n",
       "      <td>78</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>0.499999</td>\n",
       "      <td>0.797957</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-29</td>\n",
       "      <td>576</td>\n",
       "      <td>81</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.427053</td>\n",
       "      <td>0.734381</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.999956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>576</td>\n",
       "      <td>50</td>\n",
       "      <td>0.086806</td>\n",
       "      <td>0.544726</td>\n",
       "      <td>0.826490</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>576</td>\n",
       "      <td>7</td>\n",
       "      <td>0.012153</td>\n",
       "      <td>0.551168</td>\n",
       "      <td>0.833077</td>\n",
       "      <td>0.864172</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.864172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>576</td>\n",
       "      <td>9</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.479401</td>\n",
       "      <td>0.781320</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-12-03</td>\n",
       "      <td>576</td>\n",
       "      <td>41</td>\n",
       "      <td>0.071181</td>\n",
       "      <td>0.398783</td>\n",
       "      <td>0.708626</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.999980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-12-04</td>\n",
       "      <td>576</td>\n",
       "      <td>46</td>\n",
       "      <td>0.079861</td>\n",
       "      <td>0.393049</td>\n",
       "      <td>0.712068</td>\n",
       "      <td>0.757414</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.757414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-12-05</td>\n",
       "      <td>576</td>\n",
       "      <td>13</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>0.799057</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.999978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-12-06</td>\n",
       "      <td>576</td>\n",
       "      <td>11</td>\n",
       "      <td>0.019097</td>\n",
       "      <td>0.422288</td>\n",
       "      <td>0.737457</td>\n",
       "      <td>0.777275</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-12-07</td>\n",
       "      <td>576</td>\n",
       "      <td>87</td>\n",
       "      <td>0.151042</td>\n",
       "      <td>0.547525</td>\n",
       "      <td>0.830960</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-12-08</td>\n",
       "      <td>576</td>\n",
       "      <td>76</td>\n",
       "      <td>0.131944</td>\n",
       "      <td>0.553631</td>\n",
       "      <td>0.838058</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.999967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-12-09</td>\n",
       "      <td>576</td>\n",
       "      <td>68</td>\n",
       "      <td>0.118056</td>\n",
       "      <td>0.479989</td>\n",
       "      <td>0.781440</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-12-10</td>\n",
       "      <td>576</td>\n",
       "      <td>40</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.397876</td>\n",
       "      <td>0.705410</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.999949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207327</td>\n",
       "      <td>0.290808</td>\n",
       "      <td>0.297370</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_utc  n_hours  n_pos_hours  pos_rate_hours  p_hat_mean  p_hat_p95  \\\n",
       "1   2025-11-28      576           78        0.135417    0.499999   0.797957   \n",
       "2   2025-11-29      576           81        0.140625    0.427053   0.734381   \n",
       "3   2025-11-30      576           50        0.086806    0.544726   0.826490   \n",
       "4   2025-12-01      576            7        0.012153    0.551168   0.833077   \n",
       "5   2025-12-02      576            9        0.015625    0.479401   0.781320   \n",
       "6   2025-12-03      576           41        0.071181    0.398783   0.708626   \n",
       "7   2025-12-04      576           46        0.079861    0.393049   0.712068   \n",
       "8   2025-12-05      576           13        0.022569    0.499910   0.799057   \n",
       "9   2025-12-06      576           11        0.019097    0.422288   0.737457   \n",
       "10  2025-12-07      576           87        0.151042    0.547525   0.830960   \n",
       "11  2025-12-08      576           76        0.131944    0.553631   0.838058   \n",
       "12  2025-12-09      576           68        0.118056    0.479989   0.781440   \n",
       "13  2025-12-10      576           40        0.069444    0.397876   0.705410   \n",
       "14  2025-12-11        7            0        0.000000    0.207327   0.290808   \n",
       "\n",
       "    p_hat_max  n_assets  n_alert_assets  n_alert_asset_days_positive  \\\n",
       "1    0.999991        24               5                            5   \n",
       "2    0.999956        24               5                            3   \n",
       "3    0.999972        24               5                            2   \n",
       "4    0.864172        24               5                            0   \n",
       "5    0.999972        24               5                            1   \n",
       "6    0.999980        24               5                            3   \n",
       "7    0.757414        24               5                            1   \n",
       "8    0.999978        24               5                            1   \n",
       "9    0.777275        24               5                            0   \n",
       "10   0.999993        24               5                            5   \n",
       "11   0.999967        24               5                            4   \n",
       "12   0.999990        24               5                            3   \n",
       "13   0.999949        24               5                            2   \n",
       "14   0.297370         7               5                            0   \n",
       "\n",
       "    precision_at_k  max_alert_p_hat  \n",
       "1              1.0         0.999991  \n",
       "2              0.6         0.999956  \n",
       "3              0.4         0.999972  \n",
       "4              0.0         0.864172  \n",
       "5              0.2         0.999972  \n",
       "6              0.6         0.999980  \n",
       "7              0.2         0.757414  \n",
       "8              0.2         0.999978  \n",
       "9              0.0         0.777275  \n",
       "10             1.0         0.999993  \n",
       "11             0.8         0.999967  \n",
       "12             0.6         0.999990  \n",
       "13             0.4         0.999949  \n",
       "14             0.0         0.297370  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 5 — Dashboard bundle exports (small + app-friendly):\n",
    "#   Build lightweight KPI JSON + “latest day” alert queue + top sites/lines\n",
    "#   Update DASHBOARD_EXPORTS.json with the new bundle artifacts\n",
    "#============================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Locate the active EXPORT_DIR + manifest (created in Cell 1)\n",
    "# -----------------------------\n",
    "if \"EXPORT_DIR\" not in globals():\n",
    "    raise NameError(\"EXPORT_DIR is not defined. Please run Cell 1 first.\")\n",
    "\n",
    "EXPORT_DIR = Path(EXPORT_DIR)\n",
    "manifest_path = EXPORT_DIR / \"DASHBOARD_EXPORTS.json\"\n",
    "if not manifest_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing manifest: {manifest_path}\")\n",
    "\n",
    "manifest = json.loads(manifest_path.read_text())\n",
    "\n",
    "# Ensure manifest[\"exports\"] is a dict (older cells may have initialized it differently)\n",
    "exports_obj = manifest.get(\"exports\", {})\n",
    "if isinstance(exports_obj, list):\n",
    "    # Convert list -> dict using basename keys (best-effort)\n",
    "    exports_obj = {Path(p).name: p for p in exports_obj if isinstance(p, str)}\n",
    "elif not isinstance(exports_obj, dict):\n",
    "    exports_obj = {}\n",
    "manifest[\"exports\"] = exports_obj\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Inputs produced by earlier cells in this notebook\n",
    "# -----------------------------\n",
    "alerts_with_why_path = EXPORT_DIR / \"panel_alerts_top5_assets_per_day_test_with_why.csv\"\n",
    "daily_rollup_path    = EXPORT_DIR / \"panel_daily_rollup_test.csv\"\n",
    "site_day_rollup_path = EXPORT_DIR / \"panel_site_day_rollup_test.csv\"\n",
    "line_day_rollup_path = EXPORT_DIR / \"panel_line_day_rollup_test.csv\"\n",
    "budget_metrics_path  = EXPORT_DIR / \"panel_alert_budget_top5_metrics_test.json\"\n",
    "\n",
    "for p in [alerts_with_why_path, daily_rollup_path, site_day_rollup_path, line_day_rollup_path, budget_metrics_path]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing expected input from earlier cells: {p}\")\n",
    "\n",
    "alerts = pd.read_csv(alerts_with_why_path)\n",
    "daily  = pd.read_csv(daily_rollup_path)\n",
    "site_d = pd.read_csv(site_day_rollup_path)\n",
    "line_d = pd.read_csv(line_day_rollup_path)\n",
    "budget_metrics = json.loads(budget_metrics_path.read_text())\n",
    "\n",
    "# Normalize dates/timestamps (robust)\n",
    "if \"date_utc\" in alerts.columns:\n",
    "    alerts[\"date_utc\"] = pd.to_datetime(alerts[\"date_utc\"], errors=\"coerce\").dt.date\n",
    "if \"ts_peak\" in alerts.columns:\n",
    "    alerts[\"ts_peak\"] = pd.to_datetime(alerts[\"ts_peak\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "if \"date_utc\" in daily.columns:\n",
    "    daily[\"date_utc\"] = pd.to_datetime(daily[\"date_utc\"], errors=\"coerce\").dt.date\n",
    "if \"date_utc\" in site_d.columns:\n",
    "    site_d[\"date_utc\"] = pd.to_datetime(site_d[\"date_utc\"], errors=\"coerce\").dt.date\n",
    "if \"date_utc\" in line_d.columns:\n",
    "    line_d[\"date_utc\"] = pd.to_datetime(line_d[\"date_utc\"], errors=\"coerce\").dt.date\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Determine “latest day” in TEST exports and create app-friendly slices\n",
    "# -----------------------------\n",
    "if alerts.empty:\n",
    "    raise ValueError(\"alerts_with_why is empty; cannot build dashboard bundle.\")\n",
    "\n",
    "latest_day = max([d for d in alerts[\"date_utc\"].dropna().tolist()])\n",
    "alerts_latest = alerts.loc[alerts[\"date_utc\"] == latest_day].copy()\n",
    "\n",
    "# Enforce the budget view (Top-5 assets/day) and keep columns that are UI-friendly\n",
    "alert_cols = [\n",
    "    \"date_utc\", \"asset_id\", \"ts_peak\", \"p_hat\", \"y_asset_day\",\n",
    "    \"site_id\", \"line_id\", \"asset_type\", \"is_legacy\",\n",
    "    \"why_push_to_1\", \"why_push_to_0\",\n",
    "]\n",
    "alert_cols = [c for c in alert_cols if c in alerts_latest.columns]\n",
    "alerts_latest = alerts_latest[alert_cols].sort_values(\"p_hat\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Top sites/lines for latest day (choose a stable “risk” sort key if present)\n",
    "def _topn(df: pd.DataFrame, group_cols: list, n: int = 10) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # prefer max risk, else p95, else mean\n",
    "    sort_key = None\n",
    "    for k in [\"p_hat_max\", \"p_hat_p95\", \"p_hat_mean\"]:\n",
    "        if k in df.columns:\n",
    "            sort_key = k\n",
    "            break\n",
    "    if sort_key is None:\n",
    "        sort_key = df.columns[-1]  # last resort\n",
    "    out = df.sort_values(sort_key, ascending=False).head(n).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "site_latest = site_d.loc[site_d[\"date_utc\"] == latest_day].copy()\n",
    "line_latest = line_d.loc[line_d[\"date_utc\"] == latest_day].copy()\n",
    "\n",
    "top_sites_latest = _topn(site_latest, group_cols=[\"site_id\"], n=10)\n",
    "top_lines_latest = _topn(line_latest, group_cols=[\"line_id\"], n=10)\n",
    "\n",
    "# Trend window (last 14 days available in TEST range)\n",
    "daily_sorted = daily.dropna(subset=[\"date_utc\"]).sort_values(\"date_utc\").copy()\n",
    "trend_tail = daily_sorted.tail(14).copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 3) KPI summary (small JSON) for the dashboard header\n",
    "# -----------------------------\n",
    "date_min = daily_sorted[\"date_utc\"].min()\n",
    "date_max = daily_sorted[\"date_utc\"].max()\n",
    "\n",
    "kpis = {\n",
    "    \"export_run_id\": manifest.get(\"export_run_id\"),\n",
    "    \"panel_run_dir\": manifest.get(\"panel_run_dir\"),\n",
    "    \"scope\": \"TEST\",\n",
    "    \"date_min_utc\": str(date_min) if pd.notna(date_min) else None,\n",
    "    \"date_max_utc\": str(date_max) if pd.notna(date_max) else None,\n",
    "    \"n_days\": int(daily_sorted[\"date_utc\"].nunique()) if not daily_sorted.empty else 0,\n",
    "    \"n_assets_test\": int(alerts[\"asset_id\"].nunique()) if \"asset_id\" in alerts.columns else None,\n",
    "    \"alerts_budget\": {\n",
    "        \"policy\": budget_metrics.get(\"policy\", \"top_k_assets_per_day\"),\n",
    "        \"k_assets_per_day\": budget_metrics.get(\"k\"),\n",
    "        \"precision_at_k_asset_day\": budget_metrics.get(\"precision_at_k_asset_day\"),\n",
    "        \"note\": budget_metrics.get(\"note\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Save “dashboard bundle” artifacts\n",
    "# -----------------------------\n",
    "alerts_latest_out = EXPORT_DIR / \"dashboard_alert_queue_latest_day_test.csv\"\n",
    "top_sites_out     = EXPORT_DIR / \"dashboard_top_sites_latest_day_test.csv\"\n",
    "top_lines_out     = EXPORT_DIR / \"dashboard_top_lines_latest_day_test.csv\"\n",
    "trend_out         = EXPORT_DIR / \"dashboard_daily_trend_last14_test.csv\"\n",
    "kpis_out          = EXPORT_DIR / \"dashboard_kpis_test.json\"\n",
    "\n",
    "alerts_latest.to_csv(alerts_latest_out, index=False)\n",
    "top_sites_latest.to_csv(top_sites_out, index=False)\n",
    "top_lines_latest.to_csv(top_lines_out, index=False)\n",
    "trend_tail.to_csv(trend_out, index=False)\n",
    "kpis_out.write_text(json.dumps(kpis, indent=2))\n",
    "\n",
    "# Update manifest with these paths\n",
    "manifest[\"exports\"][\"dashboard_alert_queue_latest_day_test_csv\"] = str(alerts_latest_out)\n",
    "manifest[\"exports\"][\"dashboard_top_sites_latest_day_test_csv\"]   = str(top_sites_out)\n",
    "manifest[\"exports\"][\"dashboard_top_lines_latest_day_test_csv\"]   = str(top_lines_out)\n",
    "manifest[\"exports\"][\"dashboard_daily_trend_last14_test_csv\"]     = str(trend_out)\n",
    "manifest[\"exports\"][\"dashboard_kpis_test_json\"]                  = str(kpis_out)\n",
    "\n",
    "manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "print(\"Latest day (UTC):\", latest_day)\n",
    "print(\"\\nSaved dashboard bundle:\")\n",
    "print(\" \", alerts_latest_out)\n",
    "print(\" \", top_sites_out)\n",
    "print(\" \", top_lines_out)\n",
    "print(\" \", trend_out)\n",
    "print(\" \", kpis_out)\n",
    "print(\"\\nUpdated manifest:\", manifest_path)\n",
    "\n",
    "print(\"\\nAlert queue (latest day) preview:\")\n",
    "display(alerts_latest)\n",
    "\n",
    "print(\"\\nTop sites (latest day) preview:\")\n",
    "display(top_sites_latest.head(10))\n",
    "\n",
    "print(\"\\nTop lines (latest day) preview:\")\n",
    "display(top_lines_latest.head(10))\n",
    "\n",
    "print(\"\\nDaily trend (last 14 days) preview:\")\n",
    "display(trend_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92868889-04d0-45bd-9b11-75a2c8452122",
   "metadata": {},
   "source": [
    "### What Cell 5 Just Did — Dashboard bundle exports (small + app-friendly)\n",
    "\n",
    "This cell takes the **actionability outputs from Cells 2–4** and packages them into a **lightweight “dashboard bundle”** that’s easy for an API/UI to consume without loading large parquet tables.\n",
    "\n",
    "**Inputs used (from this export run directory):**\n",
    "- `panel_alerts_top5_assets_per_day_test_with_why.csv` (Top-5 assets/day with “why” strings)\n",
    "- `panel_daily_rollup_test.csv`, `panel_site_day_rollup_test.csv`, `panel_line_day_rollup_test.csv` (rollups)\n",
    "- `panel_alert_budget_top5_metrics_test.json` (alerts budget metrics)\n",
    "\n",
    "**What it produced:**\n",
    "1. **Latest-day alert queue (Top-5 assets/day)**  \n",
    "   - Picked the **latest available TEST day**: **2025-12-11 (UTC)**  \n",
    "   - Built an alert queue containing **exactly 5 assets** for that day, ordered by `p_hat`, and kept the “why” fields:\n",
    "     - `why_push_to_1` (top positive contributions driving risk up)\n",
    "     - `why_push_to_0` (top negative contributions pushing risk down)\n",
    "   - In this run, the **Top-5 queue is relatively low-risk** (max `p_hat` ≈ **0.297**) and **none of the alerted asset-days had a positive hour** (`y_asset_day = 0` for all 5).\n",
    "\n",
    "2. **Latest-day top Sites and Lines (KPI slices)**  \n",
    "   - Generated “latest day” slices for:\n",
    "     - Sites: **S1, S4, S3**\n",
    "     - Lines: **S1-L3, S4-L3, S3-L5, S1-L4, S1-L2**\n",
    "   - These tables are sized for UI cards (small and fast).\n",
    "\n",
    "3. **Last-14-day daily trend window**  \n",
    "   - Exported the **last 14 available TEST days** from the daily rollup for plotting trend lines (volume, positivity, risk percentiles, max alert risk, etc.).\n",
    "   - The trend table shows that earlier days have **high max-alert probabilities** near 1.0, while **2025-12-11** is a partial day (only **7 hours**) with much lower max risk (~0.297).\n",
    "\n",
    "4. **KPI JSON for a dashboard header**  \n",
    "   - Wrote a compact JSON summary with:\n",
    "     - export run id + source panel run dir\n",
    "     - TEST date range\n",
    "     - alerts budget policy (`top_k_assets_per_day`, **k=5**)\n",
    "     - `precision_at_k_asset_day` (from the saved budget metrics)\n",
    "\n",
    "**Saved artifacts:**\n",
    "- `dashboard_alert_queue_latest_day_test.csv`\n",
    "- `dashboard_top_sites_latest_day_test.csv`\n",
    "- `dashboard_top_lines_latest_day_test.csv`\n",
    "- `dashboard_daily_trend_last14_test.csv`\n",
    "- `dashboard_kpis_test.json`\n",
    "\n",
    "**Provenance tracking:**\n",
    "- Updated `DASHBOARD_EXPORTS.json` so downstream steps (API/dashboard code) can reliably discover these exported files by name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c10bc32-6e9c-4973-ac62-6438ff87b6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using EXPORT_DIR: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z\n",
      "Figures dir: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/figures\n",
      "Manifest: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json\n",
      "\n",
      "Loaded dashboard tables:\n",
      "  alert_queue rows: 5\n",
      "  top_sites rows  : 3\n",
      "  top_lines rows  : 5\n",
      "  trend rows      : 14\n",
      "  trend date range: 2025-11-28 00:00:00 → 2025-12-11 00:00:00\n",
      "Saved: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/figures/daily_risk_trend_test.png\n",
      "Saved: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/figures/daily_positive_hours_test.png\n",
      "Saved: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/figures/alert_queue_latest_day_test.png\n",
      "Saved: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/figures/top_sites_latest_day_test.png\n",
      "\n",
      "Saved bundle: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/dashboard_bundle_test.zip\n",
      "Updated manifest: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json\n",
      "\n",
      "Manifest exports (new/updated):\n",
      "  - fig_daily_risk_trend_test_png: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/figures/daily_risk_trend_test.png\n",
      "  - fig_daily_positive_hours_test_png: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/figures/daily_positive_hours_test.png\n",
      "  - fig_alert_queue_latest_day_test_png: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/figures/alert_queue_latest_day_test.png\n",
      "  - fig_top_sites_latest_day_test_png: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/figures/top_sites_latest_day_test.png\n",
      "  - dashboard_bundle_test_zip: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/dashboard_bundle_test.zip\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 6 — Dashboard visuals + packaged bundle (TEST exports)\n",
    "#   - Create a small set of PNG charts for quick inspection / UI prototyping\n",
    "#   - Zip the dashboard bundle (CSVs + JSON + figures) for easy sharing\n",
    "#   - Update DASHBOARD_EXPORTS.json with figure + bundle paths\n",
    "#============================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Locate the current EXPORT_DIR + manifest\n",
    "# -----------------------------\n",
    "manifest_path = EXPORT_DIR / \"DASHBOARD_EXPORTS.json\"\n",
    "assert manifest_path.exists(), f\"Missing manifest: {manifest_path}\"\n",
    "\n",
    "manifest = json.loads(manifest_path.read_text())\n",
    "\n",
    "# Defensive: ensure manifest[\"exports\"] is a dict (some earlier versions accidentally wrote a list)\n",
    "if not isinstance(manifest.get(\"exports\", {}), dict):\n",
    "    manifest[\"exports\"] = {}\n",
    "\n",
    "# Figures directory inside this export run\n",
    "FIG_DIR = EXPORT_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Using EXPORT_DIR:\", EXPORT_DIR)\n",
    "print(\"Figures dir:\", FIG_DIR)\n",
    "print(\"Manifest:\", manifest_path)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load the “dashboard bundle” tables created in Cell 5\n",
    "# -----------------------------\n",
    "alert_queue_path = EXPORT_DIR / \"dashboard_alert_queue_latest_day_test.csv\"\n",
    "top_sites_path   = EXPORT_DIR / \"dashboard_top_sites_latest_day_test.csv\"\n",
    "top_lines_path   = EXPORT_DIR / \"dashboard_top_lines_latest_day_test.csv\"\n",
    "trend_path       = EXPORT_DIR / \"dashboard_daily_trend_last14_test.csv\"\n",
    "kpis_path        = EXPORT_DIR / \"dashboard_kpis_test.json\"\n",
    "\n",
    "for p in [alert_queue_path, top_sites_path, top_lines_path, trend_path, kpis_path]:\n",
    "    assert p.exists(), f\"Missing required dashboard export: {p}\"\n",
    "\n",
    "alert_q  = pd.read_csv(alert_queue_path)\n",
    "top_sites = pd.read_csv(top_sites_path)\n",
    "top_lines = pd.read_csv(top_lines_path)\n",
    "trend = pd.read_csv(trend_path)\n",
    "kpis = json.loads(Path(kpis_path).read_text())\n",
    "\n",
    "# Parse dates for plotting\n",
    "if \"date_utc\" in trend.columns:\n",
    "    trend[\"date_utc\"] = pd.to_datetime(trend[\"date_utc\"], errors=\"coerce\")\n",
    "if \"date_utc\" in alert_q.columns:\n",
    "    alert_q[\"date_utc\"] = pd.to_datetime(alert_q[\"date_utc\"], errors=\"coerce\")\n",
    "if \"date_utc\" in top_sites.columns:\n",
    "    top_sites[\"date_utc\"] = pd.to_datetime(top_sites[\"date_utc\"], errors=\"coerce\")\n",
    "if \"date_utc\" in top_lines.columns:\n",
    "    top_lines[\"date_utc\"] = pd.to_datetime(top_lines[\"date_utc\"], errors=\"coerce\")\n",
    "\n",
    "# Lightweight sanity prints (helps confirm we’re charting the intended window)\n",
    "print(\"\\nLoaded dashboard tables:\")\n",
    "print(\"  alert_queue rows:\", len(alert_q))\n",
    "print(\"  top_sites rows  :\", len(top_sites))\n",
    "print(\"  top_lines rows  :\", len(top_lines))\n",
    "print(\"  trend rows      :\", len(trend))\n",
    "print(\"  trend date range:\", trend[\"date_utc\"].min(), \"→\", trend[\"date_utc\"].max())\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Plot A — Daily max alert probability (and mean risk) over time\n",
    "# -----------------------------\n",
    "fig_a = FIG_DIR / \"daily_risk_trend_test.png\"\n",
    "\n",
    "plt.figure()\n",
    "# Always sort by date for time series plots\n",
    "trend_sorted = trend.sort_values(\"date_utc\").copy()\n",
    "\n",
    "# Use columns if present (robust to minor schema changes)\n",
    "x = trend_sorted[\"date_utc\"]\n",
    "if \"max_alert_p_hat\" in trend_sorted.columns:\n",
    "    plt.plot(x, trend_sorted[\"max_alert_p_hat\"], label=\"Max alert p_hat\")\n",
    "if \"p_hat_mean\" in trend_sorted.columns:\n",
    "    plt.plot(x, trend_sorted[\"p_hat_mean\"], label=\"Mean p_hat\")\n",
    "\n",
    "plt.title(\"TEST — Daily risk trend (max alert + mean)\")\n",
    "plt.xlabel(\"UTC date\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_a, dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved:\", fig_a)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Plot B — Daily incident activity (positive hours vs total hours)\n",
    "# -----------------------------\n",
    "fig_b = FIG_DIR / \"daily_positive_hours_test.png\"\n",
    "\n",
    "plt.figure()\n",
    "x = trend_sorted[\"date_utc\"]\n",
    "if \"n_pos_hours\" in trend_sorted.columns and \"n_hours\" in trend_sorted.columns:\n",
    "    plt.plot(x, trend_sorted[\"n_pos_hours\"], label=\"Positive hours\")\n",
    "    plt.plot(x, trend_sorted[\"n_hours\"], label=\"Total hours\")\n",
    "elif \"pos_rate_hours\" in trend_sorted.columns:\n",
    "    plt.plot(x, trend_sorted[\"pos_rate_hours\"], label=\"Positive rate (hours)\")\n",
    "else:\n",
    "    # Fallback: nothing to plot (should be rare), but don’t hard-fail\n",
    "    plt.text(0.5, 0.5, \"No activity columns found\", ha=\"center\", va=\"center\")\n",
    "plt.title(\"TEST — Daily activity (positive hours vs total)\")\n",
    "plt.xlabel(\"UTC date\")\n",
    "plt.ylabel(\"Count (hours)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_b, dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved:\", fig_b)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Plot C — Latest-day alert queue (Top-5 assets) as a bar chart\n",
    "# -----------------------------\n",
    "fig_c = FIG_DIR / \"alert_queue_latest_day_test.png\"\n",
    "\n",
    "plt.figure()\n",
    "# Ensure highest risk at top (left-to-right) for readability\n",
    "aq = alert_q.sort_values(\"p_hat\", ascending=False).copy()\n",
    "labels = aq[\"asset_id\"].astype(str).tolist()\n",
    "vals = aq[\"p_hat\"].astype(float).tolist()\n",
    "\n",
    "plt.bar(labels, vals)\n",
    "plt.title(\"TEST — Alert queue (latest day, Top-5 assets)\")\n",
    "plt.xlabel(\"Asset ID\")\n",
    "plt.ylabel(\"p_hat (peak hour)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_c, dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved:\", fig_c)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Plot D — Latest-day top sites (max probability by site)\n",
    "# -----------------------------\n",
    "fig_d = FIG_DIR / \"top_sites_latest_day_test.png\"\n",
    "\n",
    "plt.figure()\n",
    "ts = top_sites.sort_values(\"p_hat_max\", ascending=False).copy() if \"p_hat_max\" in top_sites.columns else top_sites.copy()\n",
    "site_labels = ts[\"site_id\"].astype(str).tolist()\n",
    "site_vals = ts[\"p_hat_max\"].astype(float).tolist() if \"p_hat_max\" in ts.columns else ts[\"p_hat_mean\"].astype(float).tolist()\n",
    "\n",
    "plt.bar(site_labels, site_vals)\n",
    "plt.title(\"TEST — Top sites (latest day)\")\n",
    "plt.xlabel(\"Site\")\n",
    "plt.ylabel(\"Risk (max p_hat)\" if \"p_hat_max\" in ts.columns else \"Risk (mean p_hat)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_d, dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved:\", fig_d)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Update manifest with figure paths + create a zip bundle\n",
    "# -----------------------------\n",
    "# Register figures in the manifest\n",
    "manifest[\"exports\"][\"fig_daily_risk_trend_test_png\"] = str(fig_a)\n",
    "manifest[\"exports\"][\"fig_daily_positive_hours_test_png\"] = str(fig_b)\n",
    "manifest[\"exports\"][\"fig_alert_queue_latest_day_test_png\"] = str(fig_c)\n",
    "manifest[\"exports\"][\"fig_top_sites_latest_day_test_png\"] = str(fig_d)\n",
    "\n",
    "# Create a small zip bundle with:\n",
    "# - the “dashboard bundle” CSVs + KPI JSON\n",
    "# - the figures\n",
    "# - the manifest itself\n",
    "zip_out = EXPORT_DIR / \"dashboard_bundle_test.zip\"\n",
    "\n",
    "bundle_files = [\n",
    "    alert_queue_path,\n",
    "    top_sites_path,\n",
    "    top_lines_path,\n",
    "    trend_path,\n",
    "    kpis_path,\n",
    "    fig_a,\n",
    "    fig_b,\n",
    "    fig_c,\n",
    "    fig_d,\n",
    "    manifest_path,\n",
    "]\n",
    "\n",
    "with zipfile.ZipFile(zip_out, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fp in bundle_files:\n",
    "        fp = Path(fp)\n",
    "        if fp.exists():\n",
    "            # Store inside the zip relative to EXPORT_DIR (clean paths)\n",
    "            zf.write(fp, arcname=str(fp.relative_to(EXPORT_DIR)))\n",
    "\n",
    "manifest[\"exports\"][\"dashboard_bundle_test_zip\"] = str(zip_out)\n",
    "\n",
    "# Persist updated manifest\n",
    "manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "print(\"\\nSaved bundle:\", zip_out)\n",
    "print(\"Updated manifest:\", manifest_path)\n",
    "\n",
    "# Quick peek: show what we registered\n",
    "print(\"\\nManifest exports (new/updated):\")\n",
    "for k in [\n",
    "    \"fig_daily_risk_trend_test_png\",\n",
    "    \"fig_daily_positive_hours_test_png\",\n",
    "    \"fig_alert_queue_latest_day_test_png\",\n",
    "    \"fig_top_sites_latest_day_test_png\",\n",
    "    \"dashboard_bundle_test_zip\",\n",
    "]:\n",
    "    print(f\"  - {k}: {manifest['exports'].get(k)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869dcb34-2808-4ed0-84a1-510cce619896",
   "metadata": {},
   "source": [
    "### What Cell 6 Just Did — Dashboard visuals + packaged export bundle (TEST)\n",
    "\n",
    "This cell turned the “dashboard-ready” tables from **Cell 5** into a lightweight, shareable dashboard export package.\n",
    "\n",
    "**Inputs loaded from the current export run (`EXPORT_DIR`):**\n",
    "- `dashboard_alert_queue_latest_day_test.csv` (5 rows — the Top-5 assets for the latest day)\n",
    "- `dashboard_top_sites_latest_day_test.csv` (3 rows)\n",
    "- `dashboard_top_lines_latest_day_test.csv` (5 rows)\n",
    "- `dashboard_daily_trend_last14_test.csv` (14 rows — **2025-11-28 → 2025-12-11 UTC**)\n",
    "- `dashboard_kpis_test.json`\n",
    "- Existing manifest: `DASHBOARD_EXPORTS.json`\n",
    "\n",
    "**Visuals generated (saved to `figures/`):**\n",
    "- `daily_risk_trend_test.png`  \n",
    "  Plots the daily risk trend using:\n",
    "  - `max_alert_p_hat` (max peak-hour alert probability that day), and\n",
    "  - `p_hat_mean` (mean predicted risk across all scored hours that day).\n",
    "- `daily_positive_hours_test.png`  \n",
    "  Plots daily label activity using:\n",
    "  - `n_pos_hours` (hours labeled positive), and\n",
    "  - `n_hours` (total scored hours).\n",
    "- `alert_queue_latest_day_test.png`  \n",
    "  Bar chart of the **latest day’s Top-5 assets** by `p_hat` (peak-hour risk).\n",
    "- `top_sites_latest_day_test.png`  \n",
    "  Bar chart of **latest day’s top sites** using `p_hat_max` (or `p_hat_mean` if max is unavailable).\n",
    "\n",
    "**Packaging + provenance:**\n",
    "- Created a single zip bundle for easy sharing and downstream ingestion:\n",
    "  - `dashboard_bundle_test.zip`  \n",
    "  This includes the dashboard CSVs, KPI JSON, the PNG figures, and the manifest itself.\n",
    "\n",
    "**Manifest update:**\n",
    "- Updated `DASHBOARD_EXPORTS.json` with the new figure paths and the zip bundle path, so downstream code (API/UI) can reliably discover the latest export artifacts.\n",
    "\n",
    "**Key outputs produced:**\n",
    "- Figures:\n",
    "  - `figures/daily_risk_trend_test.png`\n",
    "  - `figures/daily_positive_hours_test.png`\n",
    "  - `figures/alert_queue_latest_day_test.png`\n",
    "  - `figures/top_sites_latest_day_test.png`\n",
    "- Bundle:\n",
    "  - `dashboard_bundle_test.zip`\n",
    "- Updated manifest:\n",
    "  - `DASHBOARD_EXPORTS.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c53b04f-9cbd-4008-98ed-293cee2adf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json\n",
      "\n",
      "Publishing to stable directory:\n",
      "  LATEST_DIR: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/latest_test\n",
      "\n",
      "Published files:\n",
      "  ✅ dashboard_alert_queue_latest_day_test.csv  →  dashboard_alert_queue_latest_day_test.csv\n",
      "  ✅ dashboard_top_sites_latest_day_test.csv  →  dashboard_top_sites_latest_day_test.csv\n",
      "  ✅ dashboard_top_lines_latest_day_test.csv  →  dashboard_top_lines_latest_day_test.csv\n",
      "  ✅ dashboard_daily_trend_last14_test.csv  →  dashboard_daily_trend_last14_test.csv\n",
      "  ✅ dashboard_kpis_test.json  →  dashboard_kpis_test.json\n",
      "  ✅ figures/daily_risk_trend_test.png  →  daily_risk_trend_test.png\n",
      "  ✅ figures/daily_positive_hours_test.png  →  daily_positive_hours_test.png\n",
      "  ✅ figures/alert_queue_latest_day_test.png  →  alert_queue_latest_day_test.png\n",
      "  ✅ figures/top_sites_latest_day_test.png  →  top_sites_latest_day_test.png\n",
      "  ✅ dashboard_bundle_test.zip  →  dashboard_bundle_test.zip\n",
      "  ✅ DASHBOARD_EXPORTS.json  →  DASHBOARD_EXPORTS.json\n",
      "\n",
      "Wrote pointer: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/latest_test/LATEST_TEST.json\n",
      "Updated repo exports index: /home/parallels/projects/gmp-packaging-risk-analytics/EXPORTS.json\n",
      "Updated run manifest with published_latest: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json\n",
      "\n",
      "LATEST_DIR listing:\n",
      "  DASHBOARD_EXPORTS.json\n",
      "  LATEST_TEST.json\n",
      "  alert_queue_latest_day_test.png\n",
      "  daily_positive_hours_test.png\n",
      "  daily_risk_trend_test.png\n",
      "  dashboard_alert_queue_latest_day_test.csv\n",
      "  dashboard_bundle_test.zip\n",
      "  dashboard_daily_trend_last14_test.csv\n",
      "  dashboard_kpis_test.json\n",
      "  dashboard_top_lines_latest_day_test.csv\n",
      "  dashboard_top_sites_latest_day_test.csv\n",
      "  top_sites_latest_day_test.png\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 7 — Publish “latest” TEST dashboard artifacts + update repo EXPORTS.json\n",
    "#   Goal: create stable, API-friendly paths (risk_scoring/latest_test/...) that always point to the most recent export run\n",
    "#============================================================\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Preconditions (from Cell 1)\n",
    "# -----------------------------\n",
    "assert \"ROOT\" in globals(), \"Expected ROOT from Cell 1\"\n",
    "assert \"RS_DIR\" in globals(), \"Expected RS_DIR from Cell 1\"\n",
    "assert \"EXPORT_DIR\" in globals(), \"Expected EXPORT_DIR from Cell 1\"\n",
    "assert \"EXPORT_RUN_ID\" in globals(), \"Expected EXPORT_RUN_ID from Cell 1\"\n",
    "\n",
    "ROOT = Path(ROOT)\n",
    "RS_DIR = Path(RS_DIR)\n",
    "EXPORT_DIR = Path(EXPORT_DIR)\n",
    "EXPORT_RUN_ID = str(EXPORT_RUN_ID)\n",
    "\n",
    "assert EXPORT_DIR.exists(), f\"Missing EXPORT_DIR: {EXPORT_DIR}\"\n",
    "\n",
    "manifest_path = EXPORT_DIR / \"DASHBOARD_EXPORTS.json\"\n",
    "assert manifest_path.exists(), f\"Missing manifest: {manifest_path}\"\n",
    "\n",
    "manifest = json.loads(manifest_path.read_text())\n",
    "print(\"Loaded manifest:\", manifest_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Create a stable “latest” directory for TEST artifacts\n",
    "# -----------------------------\n",
    "LATEST_DIR = RS_DIR / \"latest_test\"\n",
    "LATEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional: clean it so it truly represents \"latest\"\n",
    "for p in LATEST_DIR.glob(\"*\"):\n",
    "    if p.is_file():\n",
    "        p.unlink()\n",
    "    elif p.is_dir():\n",
    "        shutil.rmtree(p)\n",
    "\n",
    "print(\"\\nPublishing to stable directory:\")\n",
    "print(\"  LATEST_DIR:\", LATEST_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Copy the key deliverables we want the API/UI to consume\n",
    "#    Keep this *small* (CSV/JSON/PNG/ZIP), no parquet, no big matrices.\n",
    "# -----------------------------\n",
    "to_publish = [\n",
    "    # “Operational” tables\n",
    "    \"dashboard_alert_queue_latest_day_test.csv\",\n",
    "    \"dashboard_top_sites_latest_day_test.csv\",\n",
    "    \"dashboard_top_lines_latest_day_test.csv\",\n",
    "    \"dashboard_daily_trend_last14_test.csv\",\n",
    "    \"dashboard_kpis_test.json\",\n",
    "\n",
    "    # Figures\n",
    "    \"figures/daily_risk_trend_test.png\",\n",
    "    \"figures/daily_positive_hours_test.png\",\n",
    "    \"figures/alert_queue_latest_day_test.png\",\n",
    "    \"figures/top_sites_latest_day_test.png\",\n",
    "\n",
    "    # Bundle + manifest\n",
    "    \"dashboard_bundle_test.zip\",\n",
    "    \"DASHBOARD_EXPORTS.json\",\n",
    "]\n",
    "\n",
    "published = []\n",
    "missing = []\n",
    "\n",
    "for rel in to_publish:\n",
    "    src = EXPORT_DIR / rel\n",
    "    if not src.exists():\n",
    "        missing.append(rel)\n",
    "        continue\n",
    "\n",
    "    dst = LATEST_DIR / Path(rel).name  # flatten into latest_test/\n",
    "    shutil.copy2(src, dst)\n",
    "    published.append((rel, dst.name))\n",
    "\n",
    "print(\"\\nPublished files:\")\n",
    "for rel, name in published:\n",
    "    print(f\"  ✅ {rel}  →  {name}\")\n",
    "\n",
    "if missing:\n",
    "    print(\"\\nMissing (not published):\")\n",
    "    for rel in missing:\n",
    "        print(\"  ⚠️\", rel)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Write a small pointer file so humans & scripts can see what “latest” means\n",
    "# -----------------------------\n",
    "latest_pointer = {\n",
    "    \"latest_run_id\": EXPORT_RUN_ID,\n",
    "    \"latest_export_dir\": str(EXPORT_DIR),\n",
    "    \"latest_published_dir\": str(LATEST_DIR),\n",
    "}\n",
    "(LATEST_DIR / \"LATEST_TEST.json\").write_text(json.dumps(latest_pointer, indent=2))\n",
    "print(\"\\nWrote pointer:\", LATEST_DIR / \"LATEST_TEST.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Update a repo-level EXPORTS.json (small, git-safe pointers only)\n",
    "#    This is a “directory of outputs” for reviewers and your API.\n",
    "# -----------------------------\n",
    "exports_path = ROOT / \"EXPORTS.json\"\n",
    "if exports_path.exists():\n",
    "    try:\n",
    "        exports = json.loads(exports_path.read_text())\n",
    "        if not isinstance(exports, dict):\n",
    "            exports = {}\n",
    "    except Exception:\n",
    "        exports = {}\n",
    "else:\n",
    "    exports = {}\n",
    "\n",
    "exports.setdefault(\"risk_scoring\", {})\n",
    "exports[\"risk_scoring\"][\"latest_test\"] = {\n",
    "    \"run_id\": EXPORT_RUN_ID,\n",
    "    \"export_dir\": str(EXPORT_DIR),\n",
    "    \"published_dir\": str(LATEST_DIR),\n",
    "    \"files\": {name: str(LATEST_DIR / name) for _, name in published},\n",
    "    \"pointer\": str(LATEST_DIR / \"LATEST_TEST.json\"),\n",
    "}\n",
    "\n",
    "exports_path.write_text(json.dumps(exports, indent=2))\n",
    "print(\"Updated repo exports index:\", exports_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Also append a convenience block into the run manifest itself\n",
    "# -----------------------------\n",
    "manifest.setdefault(\"published_latest\", {})\n",
    "manifest[\"published_latest\"][\"latest_test_dir\"] = str(LATEST_DIR)\n",
    "manifest[\"published_latest\"][\"latest_test_pointer\"] = str(LATEST_DIR / \"LATEST_TEST.json\")\n",
    "manifest[\"published_latest\"][\"published_files\"] = [name for _, name in published]\n",
    "\n",
    "manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "print(\"Updated run manifest with published_latest:\", manifest_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Quick preview of what the API would serve\n",
    "# -----------------------------\n",
    "print(\"\\nLATEST_DIR listing:\")\n",
    "for p in sorted(LATEST_DIR.glob(\"*\")):\n",
    "    print(\" \", p.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aec27c-bba2-47b7-bc0b-a0e6f278cbdb",
   "metadata": {},
   "source": [
    "### What Cell 7 Just Did — Publish “latest” TEST dashboard exports + update `EXPORTS.json`\n",
    "\n",
    "This cell takes the **current export run** you created earlier (`EXPORT_DIR = .../risk_scoring/20251220T223649Z`) and “publishes” a **stable, always-current** TEST dashboard bundle under:\n",
    "\n",
    "- `data/processed/risk_scoring/latest_test/`\n",
    "\n",
    "It then **copies a curated set of small, API/dashboard-friendly deliverables** into that folder (CSV tables, PNG figures, ZIP bundle, and the run manifest), so downstream consumers don’t need to know the timestamped run folder name.\n",
    "\n",
    "Based on your output, the following were successfully published into `latest_test/`:\n",
    "\n",
    "- Core tables:\n",
    "  - `dashboard_alert_queue_latest_day_test.csv`\n",
    "  - `dashboard_top_sites_latest_day_test.csv`\n",
    "  - `dashboard_top_lines_latest_day_test.csv`\n",
    "  - `dashboard_daily_trend_last14_test.csv`\n",
    "  - `dashboard_kpis_test.json`\n",
    "- Key figures:\n",
    "  - `daily_risk_trend_test.png`\n",
    "  - `daily_positive_hours_test.png`\n",
    "  - `alert_queue_latest_day_test.png`\n",
    "  - `top_sites_latest_day_test.png`\n",
    "- Bundle + provenance:\n",
    "  - `dashboard_bundle_test.zip`\n",
    "  - `DASHBOARD_EXPORTS.json`\n",
    "\n",
    "To make “latest” traceable, it also wrote a pointer file:\n",
    "\n",
    "- `data/processed/risk_scoring/latest_test/LATEST_TEST.json`\n",
    "\n",
    "Finally, it updated two metadata indexes so the repo and the run both “know” where the latest published outputs live:\n",
    "\n",
    "- Repo-level pointer index:\n",
    "  - `/home/parallels/projects/gmp-packaging-risk-analytics/EXPORTS.json`\n",
    "- Run-level manifest (adds a `published_latest` section):\n",
    "  - `.../risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json`\n",
    "\n",
    "Result: your dashboard/API can now read from **one stable location** (`risk_scoring/latest_test/`) instead of hunting for the newest timestamped run directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e61ac6-5358-4737-9c40-2b188a737879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/latest_test/dashboard_payload_test.json\n",
      "Saved: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/dashboard_payload_test.json\n",
      "\n",
      "Payload sanity checks:\n",
      "  latest_day: 2025-12-11\n",
      "  alert_queue rows: 5\n",
      "  trend days: 14\n",
      "  kpis keys: ['export_run_id', 'panel_run_dir', 'scope', 'date_min_utc', 'date_max_utc', 'n_days', 'n_assets_test', 'alerts_budget'] ...\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 8 — Build an API-ready dashboard payload (TEST) from the published “latest” exports\n",
    "#   Goal:\n",
    "#     Create ONE compact JSON file that a FastAPI endpoint can return directly:\n",
    "#       - KPIs (json)\n",
    "#       - Alert queue (top 5 assets for the latest day)\n",
    "#       - Top sites/lines for the latest day\n",
    "#       - Daily trend (last 14 days)\n",
    "#     Keep it small + business-friendly (no huge tables, no parquet).\n",
    "#\n",
    "#   Inputs (from Cell 7 publish step):\n",
    "#     data/processed/risk_scoring/latest_test/\n",
    "#       - dashboard_kpis_test.json\n",
    "#       - dashboard_alert_queue_latest_day_test.csv\n",
    "#       - dashboard_top_sites_latest_day_test.csv\n",
    "#       - dashboard_top_lines_latest_day_test.csv\n",
    "#       - dashboard_daily_trend_last14_test.csv\n",
    "#       - DASHBOARD_EXPORTS.json\n",
    "#\n",
    "#   Outputs:\n",
    "#     - latest_test/dashboard_payload_test.json\n",
    "#     - (optional) also copy into EXPORT_DIR for provenance\n",
    "#============================================================\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Locate the published \"latest\" folder (TEST)\n",
    "# -----------------------------\n",
    "LATEST_DIR = RS_DIR / \"latest_test\"\n",
    "assert LATEST_DIR.exists(), f\"Missing published latest directory: {LATEST_DIR}\"\n",
    "\n",
    "manifest_latest_path = LATEST_DIR / \"DASHBOARD_EXPORTS.json\"\n",
    "kpis_path  = LATEST_DIR / \"dashboard_kpis_test.json\"\n",
    "aq_path    = LATEST_DIR / \"dashboard_alert_queue_latest_day_test.csv\"\n",
    "sites_path = LATEST_DIR / \"dashboard_top_sites_latest_day_test.csv\"\n",
    "lines_path = LATEST_DIR / \"dashboard_top_lines_latest_day_test.csv\"\n",
    "trend_path = LATEST_DIR / \"dashboard_daily_trend_last14_test.csv\"\n",
    "\n",
    "for p in [kpis_path, aq_path, sites_path, lines_path, trend_path]:\n",
    "    assert p.exists(), f\"Missing required published file: {p}\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load the published tables + KPIs\n",
    "# -----------------------------\n",
    "kpis = json.loads(kpis_path.read_text())\n",
    "\n",
    "alert_queue = pd.read_csv(aq_path)\n",
    "top_sites   = pd.read_csv(sites_path)\n",
    "top_lines   = pd.read_csv(lines_path)\n",
    "trend14     = pd.read_csv(trend_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Helper: make JSON-safe records (handles NaN, timestamps, numpy types)\n",
    "# -----------------------------\n",
    "def _clean_value(v):\n",
    "    \"\"\"Convert values into JSON-safe python primitives.\"\"\"\n",
    "    if pd.isna(v):\n",
    "        return None\n",
    "    # numpy scalar -> python scalar\n",
    "    if isinstance(v, (np.generic,)):\n",
    "        return v.item()\n",
    "    return v\n",
    "\n",
    "def df_to_records(df: pd.DataFrame, date_cols=None) -> list[dict]:\n",
    "    \"\"\"Convert df rows to list-of-dicts; optionally coerce specific columns to ISO strings.\"\"\"\n",
    "    out = df.copy()\n",
    "    date_cols = date_cols or []\n",
    "    for c in date_cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_datetime(out[c], errors=\"coerce\")\n",
    "            out[c] = out[c].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\").where(out[c].notna(), None)\n",
    "    # Replace NaN -> None and ensure python primitives\n",
    "    records = []\n",
    "    for row in out.to_dict(orient=\"records\"):\n",
    "        records.append({k: _clean_value(v) for k, v in row.items()})\n",
    "    return records\n",
    "\n",
    "# Coerce known date columns if present\n",
    "alert_queue_records = df_to_records(alert_queue, date_cols=[\"ts_peak\"])\n",
    "top_sites_records   = df_to_records(top_sites)\n",
    "top_lines_records   = df_to_records(top_lines)\n",
    "trend14_records     = df_to_records(trend14, date_cols=[\"date_utc\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Load published manifest (if present) for provenance (non-fatal)\n",
    "# -----------------------------\n",
    "manifest_latest = {}\n",
    "if manifest_latest_path.exists():\n",
    "    try:\n",
    "        manifest_latest = json.loads(manifest_latest_path.read_text())\n",
    "    except Exception:\n",
    "        manifest_latest = {}\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Assemble payload\n",
    "# -----------------------------\n",
    "created_utc = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "payload = {\n",
    "    \"kind\": \"dashboard_payload_test\",\n",
    "    \"created_utc\": created_utc,\n",
    "    \"published_latest_dir\": str(LATEST_DIR),\n",
    "    \"source_export_run_id\": manifest_latest.get(\"export_run_id\"),\n",
    "    \"source_panel_run_dir\": manifest_latest.get(\"panel_run_dir\"),\n",
    "    \"kpis\": kpis,\n",
    "    \"latest_day\": alert_queue[\"date_utc\"].iloc[0] if (\"date_utc\" in alert_queue.columns and len(alert_queue)) else None,\n",
    "    \"alert_queue_latest_day\": alert_queue_records,     # top-5 assets (budget policy)\n",
    "    \"top_sites_latest_day\": top_sites_records,         # site rollup for latest day\n",
    "    \"top_lines_latest_day\": top_lines_records,         # line rollup for latest day\n",
    "    \"daily_trend_last14\": trend14_records,             # 14-day trend window\n",
    "    \"notes\": {\n",
    "        \"scope\": \"TEST split exports only (panel model).\",\n",
    "        \"alert_budget\": \"Top-5 assets per day, selected by max p_hat within the day (asset-day risk).\",\n",
    "        \"why_fields\": \"If present, why_push_to_1 / why_push_to_0 summarize top coefficient contributions at ts_peak.\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Save payload (stable location + provenance copy)\n",
    "# -----------------------------\n",
    "payload_latest_path = LATEST_DIR / \"dashboard_payload_test.json\"\n",
    "payload_latest_path.write_text(json.dumps(payload, indent=2))\n",
    "print(\"Saved:\", payload_latest_path)\n",
    "\n",
    "# Also copy into this run's EXPORT_DIR (if defined in notebook state) for provenance\n",
    "try:\n",
    "    payload_run_path = EXPORT_DIR / \"dashboard_payload_test.json\"  # noqa: F821\n",
    "    payload_run_path.write_text(json.dumps(payload, indent=2))\n",
    "    print(\"Saved:\", payload_run_path)\n",
    "except Exception:\n",
    "    print(\"Note: EXPORT_DIR not available in this cell context; saved only to latest_test/.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Quick sanity preview\n",
    "# -----------------------------\n",
    "print(\"\\nPayload sanity checks:\")\n",
    "print(\"  latest_day:\", payload.get(\"latest_day\"))\n",
    "print(\"  alert_queue rows:\", len(payload[\"alert_queue_latest_day\"]))\n",
    "print(\"  trend days:\", len(payload[\"daily_trend_last14\"]))\n",
    "print(\"  kpis keys:\", list(payload[\"kpis\"].keys())[:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382ec97-2f64-4a96-88bb-de8dfa5afe4d",
   "metadata": {},
   "source": [
    "### What Cell 8 Just Did — API-ready dashboard payload (TEST)\n",
    "\n",
    "This cell packaged the “latest_test” dashboard exports into a single, API-friendly JSON payload that your FastAPI app can return directly.\n",
    "\n",
    "**Inputs (published stable exports from Cell 7):**\n",
    "- `data/processed/risk_scoring/latest_test/dashboard_kpis_test.json`\n",
    "- `data/processed/risk_scoring/latest_test/dashboard_alert_queue_latest_day_test.csv`\n",
    "- `data/processed/risk_scoring/latest_test/dashboard_top_sites_latest_day_test.csv`\n",
    "- `data/processed/risk_scoring/latest_test/dashboard_top_lines_latest_day_test.csv`\n",
    "- `data/processed/risk_scoring/latest_test/dashboard_daily_trend_last14_test.csv`\n",
    "- (optionally) `data/processed/risk_scoring/latest_test/DASHBOARD_EXPORTS.json` for provenance fields\n",
    "\n",
    "**What it built:**\n",
    "- A compact JSON object containing:\n",
    "  - **KPIs** (as-is from the KPI file)\n",
    "  - **latest_day** inferred from the alert queue table\n",
    "  - **alert_queue_latest_day** (Top-5 assets/day, with `ts_peak`, `p_hat`, and “why” fields if present)\n",
    "  - **top_sites_latest_day** and **top_lines_latest_day**\n",
    "  - **daily_trend_last14** (14-day trend table as JSON records)\n",
    "  - **provenance** fields like `source_export_run_id` and `source_panel_run_dir` when available\n",
    "\n",
    "**Key output artifacts created:**\n",
    "- Stable location for the API to read:\n",
    "  - `data/processed/risk_scoring/latest_test/dashboard_payload_test.json`\n",
    "- Provenance copy tied to the export run:\n",
    "  - `data/processed/risk_scoring/20251220T223649Z/dashboard_payload_test.json`\n",
    "\n",
    "**Run output recap (from this execution):**\n",
    "- Saved payload to both paths above.\n",
    "- `latest_day` resolved to **2025-12-11**\n",
    "- Alert queue contains **5 rows** (matching the Top-5 assets/day budget)\n",
    "- Daily trend contains **14 days**\n",
    "- KPI keys were confirmed loaded (e.g., `export_run_id`, `panel_run_dir`, `scope`, `date_min_utc`, `date_max_utc`, etc.)\n",
    "\n",
    "**Why this matters:**\n",
    "With this payload, your dashboard/API layer no longer needs to join multiple CSVs at request time—your endpoint can simply load and return one JSON file for a fast, deterministic response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "335e61ee-381b-4e3f-9296-95d364f20283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (stable exports): /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/latest_test\n",
      "Destination (git-friendly): /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test\n",
      "\n",
      "Copied files:\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/dashboard_payload_test.json\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/dashboard_kpis_test.json\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/dashboard_alert_queue_latest_day_test.csv\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/dashboard_top_sites_latest_day_test.csv\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/dashboard_top_lines_latest_day_test.csv\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/dashboard_daily_trend_last14_test.csv\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/DASHBOARD_EXPORTS.json\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/LATEST_TEST.json\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/figures/daily_risk_trend_test.png\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/figures/daily_positive_hours_test.png\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/figures/alert_queue_latest_day_test.png\n",
      "  ✅ /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/figures/top_sites_latest_day_test.png\n",
      "\n",
      "Wrote report: /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/DASHBOARD_REPORT_TEST.md\n",
      "\n",
      "Destination listing (reports/risk_scoring/latest_test):\n",
      "  - DASHBOARD_EXPORTS.json\n",
      "  - DASHBOARD_REPORT_TEST.md\n",
      "  - LATEST_TEST.json\n",
      "  - dashboard_alert_queue_latest_day_test.csv\n",
      "  - dashboard_daily_trend_last14_test.csv\n",
      "  - dashboard_kpis_test.json\n",
      "  - dashboard_payload_test.json\n",
      "  - dashboard_top_lines_latest_day_test.csv\n",
      "  - dashboard_top_sites_latest_day_test.csv\n",
      "\n",
      "Figures listing:\n",
      "  - figures/alert_queue_latest_day_test.png\n",
      "  - figures/daily_positive_hours_test.png\n",
      "  - figures/daily_risk_trend_test.png\n",
      "  - figures/top_sites_latest_day_test.png\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 9 — Publish a GitHub-friendly “report snapshot” (TEST)\n",
    "#   Goal:\n",
    "#     - Copy a *small*, human-readable bundle from data/processed → ./reports/\n",
    "#       so GitHub can show results without committing the full data/ directory.\n",
    "#     - Create a Markdown report that links the key CSV/JSON + figures.\n",
    "#\n",
    "#   Inputs (from Cell 7/8 “latest_test”):\n",
    "#     data/processed/risk_scoring/latest_test/*\n",
    "#\n",
    "#   Outputs (git-friendly, small artifacts):\n",
    "#     reports/risk_scoring/latest_test/\n",
    "#       - DASHBOARD_REPORT_TEST.md\n",
    "#       - dashboard_payload_test.json\n",
    "#       - dashboard_kpis_test.json\n",
    "#       - dashboard_alert_queue_latest_day_test.csv\n",
    "#       - dashboard_top_sites_latest_day_test.csv\n",
    "#       - dashboard_top_lines_latest_day_test.csv\n",
    "#       - dashboard_daily_trend_last14_test.csv\n",
    "#       - figures/*.png\n",
    "#============================================================\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Define source (stable) + destination (git-friendly) dirs\n",
    "# -----------------------------\n",
    "LATEST_DIR = ROOT / \"data\" / \"processed\" / \"risk_scoring\" / \"latest_test\"\n",
    "assert LATEST_DIR.exists(), f\"Missing LATEST_DIR: {LATEST_DIR}\"\n",
    "\n",
    "REPORTS_DIR = ROOT / \"reports\" / \"risk_scoring\" / \"latest_test\"\n",
    "FIG_DST_DIR = REPORTS_DIR / \"figures\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Source (stable exports):\", LATEST_DIR)\n",
    "print(\"Destination (git-friendly):\", REPORTS_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Define the small files we want to publish to GitHub\n",
    "# -----------------------------\n",
    "files_to_copy = [\n",
    "    \"dashboard_payload_test.json\",\n",
    "    \"dashboard_kpis_test.json\",\n",
    "    \"dashboard_alert_queue_latest_day_test.csv\",\n",
    "    \"dashboard_top_sites_latest_day_test.csv\",\n",
    "    \"dashboard_top_lines_latest_day_test.csv\",\n",
    "    \"dashboard_daily_trend_last14_test.csv\",\n",
    "    \"DASHBOARD_EXPORTS.json\",\n",
    "    \"LATEST_TEST.json\",\n",
    "]\n",
    "\n",
    "# Copy core tables/json\n",
    "copied = []\n",
    "missing = []\n",
    "for fname in files_to_copy:\n",
    "    src = LATEST_DIR / fname\n",
    "    dst = REPORTS_DIR / fname\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, dst)\n",
    "        copied.append(dst)\n",
    "    else:\n",
    "        missing.append(src)\n",
    "\n",
    "# Copy figures (PNG)\n",
    "fig_src_candidates = [\n",
    "    \"daily_risk_trend_test.png\",\n",
    "    \"daily_positive_hours_test.png\",\n",
    "    \"alert_queue_latest_day_test.png\",\n",
    "    \"top_sites_latest_day_test.png\",\n",
    "]\n",
    "for f in fig_src_candidates:\n",
    "    src = LATEST_DIR / f\n",
    "    dst = FIG_DST_DIR / f\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, dst)\n",
    "        copied.append(dst)\n",
    "    else:\n",
    "        missing.append(src)\n",
    "\n",
    "print(\"\\nCopied files:\")\n",
    "for p in copied:\n",
    "    print(\"  ✅\", p)\n",
    "\n",
    "if missing:\n",
    "    print(\"\\nMissing (not fatal):\")\n",
    "    for p in missing:\n",
    "        print(\"  ⚠️\", p)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load a few items to populate a clean Markdown report\n",
    "# -----------------------------\n",
    "payload_path = REPORTS_DIR / \"dashboard_payload_test.json\"\n",
    "kpis_path = REPORTS_DIR / \"dashboard_kpis_test.json\"\n",
    "trend_path = REPORTS_DIR / \"dashboard_daily_trend_last14_test.csv\"\n",
    "aq_path = REPORTS_DIR / \"dashboard_alert_queue_latest_day_test.csv\"\n",
    "\n",
    "payload = json.loads(payload_path.read_text()) if payload_path.exists() else {}\n",
    "kpis = json.loads(kpis_path.read_text()) if kpis_path.exists() else {}\n",
    "\n",
    "trend_df = pd.read_csv(trend_path) if trend_path.exists() else pd.DataFrame()\n",
    "aq_df = pd.read_csv(aq_path) if aq_path.exists() else pd.DataFrame()\n",
    "\n",
    "latest_day = payload.get(\"latest_day\") or (str(aq_df[\"date_utc\"].iloc[0]) if (not aq_df.empty and \"date_utc\" in aq_df.columns) else \"unknown\")\n",
    "\n",
    "# Basic quick stats for the report\n",
    "n_alerts = int(len(aq_df)) if not aq_df.empty else 0\n",
    "max_p = float(aq_df[\"p_hat\"].max()) if (not aq_df.empty and \"p_hat\" in aq_df.columns) else None\n",
    "mean_p = float(aq_df[\"p_hat\"].mean()) if (not aq_df.empty and \"p_hat\" in aq_df.columns) else None\n",
    "\n",
    "trend_days = int(trend_df[\"date_utc\"].nunique()) if (not trend_df.empty and \"date_utc\" in trend_df.columns) else 0\n",
    "trend_min = str(trend_df[\"date_utc\"].min()) if (not trend_df.empty and \"date_utc\" in trend_df.columns) else None\n",
    "trend_max = str(trend_df[\"date_utc\"].max()) if (not trend_df.empty and \"date_utc\" in trend_df.columns) else None\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Write a GitHub-readable Markdown report\n",
    "# -----------------------------\n",
    "report_md = []\n",
    "report_md.append(\"# Risk Scoring Dashboard Exports (TEST)\")\n",
    "report_md.append(\"\")\n",
    "report_md.append(f\"- **Latest day:** `{latest_day}`\")\n",
    "report_md.append(f\"- **Alerts budget:** Top-5 assets/day (queue rows = `{n_alerts}`)\")\n",
    "if mean_p is not None:\n",
    "    report_md.append(f\"- **Mean alert score (p̂):** `{mean_p:.3f}`\")\n",
    "if max_p is not None:\n",
    "    report_md.append(f\"- **Max alert score (p̂):** `{max_p:.3f}`\")\n",
    "if trend_days:\n",
    "    report_md.append(f\"- **Trend window:** `{trend_days}` days (`{trend_min}` → `{trend_max}`)\")\n",
    "report_md.append(\"\")\n",
    "report_md.append(\"## Files (API-ready)\")\n",
    "report_md.append(\"- `dashboard_payload_test.json` — single JSON payload for API responses\")\n",
    "report_md.append(\"- `dashboard_kpis_test.json` — KPI summary + provenance\")\n",
    "report_md.append(\"\")\n",
    "report_md.append(\"## Tables (CSV)\")\n",
    "report_md.append(\"- `dashboard_alert_queue_latest_day_test.csv` — Top-5 asset alerts for the latest day (+ “why”)\")\n",
    "report_md.append(\"- `dashboard_top_sites_latest_day_test.csv` — Site rollup for the latest day\")\n",
    "report_md.append(\"- `dashboard_top_lines_latest_day_test.csv` — Line rollup for the latest day\")\n",
    "report_md.append(\"- `dashboard_daily_trend_last14_test.csv` — 14-day trend table\")\n",
    "report_md.append(\"\")\n",
    "report_md.append(\"## Figures\")\n",
    "report_md.append(\"- `figures/daily_risk_trend_test.png`\")\n",
    "report_md.append(\"- `figures/daily_positive_hours_test.png`\")\n",
    "report_md.append(\"- `figures/alert_queue_latest_day_test.png`\")\n",
    "report_md.append(\"- `figures/top_sites_latest_day_test.png`\")\n",
    "report_md.append(\"\")\n",
    "report_md.append(\"## Notes\")\n",
    "report_md.append(\"- These artifacts are copied from `data/processed/risk_scoring/latest_test/` into `reports/` so they can be committed to GitHub safely.\")\n",
    "report_md.append(\"- The underlying full datasets and large intermediate artifacts remain under `data/` and stay out of git by design.\")\n",
    "report_md.append(\"\")\n",
    "report_md.append(\"## Quick Preview\")\n",
    "if not aq_df.empty:\n",
    "    preview_cols = [c for c in [\"date_utc\",\"asset_id\",\"ts_peak\",\"p_hat\",\"site_id\",\"line_id\",\"asset_type\",\"is_legacy\",\"why_push_to_1\",\"why_push_to_0\"] if c in aq_df.columns]\n",
    "    report_md.append(\"\")\n",
    "    report_md.append(\"### Alert Queue (latest day, first 5 rows)\")\n",
    "    report_md.append(\"\")\n",
    "    report_md.append(aq_df[preview_cols].head(5).to_markdown(index=False))\n",
    "else:\n",
    "    report_md.append(\"\")\n",
    "    report_md.append(\"_Alert queue preview not available (CSV missing or empty)._\")\n",
    "\n",
    "report_path = REPORTS_DIR / \"DASHBOARD_REPORT_TEST.md\"\n",
    "report_path.write_text(\"\\n\".join(report_md))\n",
    "print(\"\\nWrote report:\", report_path)\n",
    "\n",
    "print(\"\\nDestination listing (reports/risk_scoring/latest_test):\")\n",
    "for p in sorted(REPORTS_DIR.glob(\"*\")):\n",
    "    if p.is_file():\n",
    "        print(\"  -\", p.name)\n",
    "print(\"\\nFigures listing:\")\n",
    "for p in sorted(FIG_DST_DIR.glob(\"*.png\")):\n",
    "    print(\"  - figures/\" + p.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf35a7a-bebd-40b7-a3f8-7b0cd486d291",
   "metadata": {},
   "source": [
    "### What Cell 9 Just Did — Publish a GitHub-friendly “report snapshot” (TEST)\n",
    "\n",
    "This cell created a **git-safe reporting snapshot** of the latest TEST dashboard exports by copying a small, human-readable bundle from the stable processed output directory into the repo’s `reports/` folder (so results can be viewed on GitHub without committing the large `data/` tree).\n",
    "\n",
    "**Inputs used (stable “latest” exports):**\n",
    "- `data/processed/risk_scoring/latest_test/` (the published outputs from earlier cells)\n",
    "\n",
    "**What it generated:**\n",
    "- A new GitHub-friendly destination folder:\n",
    "  - `reports/risk_scoring/latest_test/`\n",
    "- A Markdown report summarizing the latest run:\n",
    "  - `reports/risk_scoring/latest_test/DASHBOARD_REPORT_TEST.md`\n",
    "- Copies of the core tables/JSON artifacts needed for review and light sharing:\n",
    "  - `dashboard_payload_test.json` (API-ready payload)\n",
    "  - `dashboard_kpis_test.json` (KPI/provenance summary)\n",
    "  - `dashboard_alert_queue_latest_day_test.csv`\n",
    "  - `dashboard_top_sites_latest_day_test.csv`\n",
    "  - `dashboard_top_lines_latest_day_test.csv`\n",
    "  - `dashboard_daily_trend_last14_test.csv`\n",
    "  - `DASHBOARD_EXPORTS.json`, `LATEST_TEST.json` (provenance pointers)\n",
    "- Copies of the key PNG figures into:\n",
    "  - `reports/risk_scoring/latest_test/figures/`\n",
    "\n",
    "**Confirmed outputs from your run:**\n",
    "- ✅ Wrote the report:\n",
    "  - `reports/risk_scoring/latest_test/DASHBOARD_REPORT_TEST.md`\n",
    "- ✅ Snapshot folder contents now include:\n",
    "  - `DASHBOARD_EXPORTS.json`\n",
    "  - `DASHBOARD_REPORT_TEST.md`\n",
    "  - `LATEST_TEST.json`\n",
    "  - `dashboard_alert_queue_latest_day_test.csv`\n",
    "  - `dashboard_daily_trend_last14_test.csv`\n",
    "  - `dashboard_kpis_test.json`\n",
    "  - `dashboard_payload_test.json`\n",
    "  - `dashboard_top_lines_latest_day_test.csv`\n",
    "  - `dashboard_top_sites_latest_day_test.csv`\n",
    "- ✅ Figures copied:\n",
    "  - `figures/alert_queue_latest_day_test.png`\n",
    "  - `figures/daily_positive_hours_test.png`\n",
    "  - `figures/daily_risk_trend_test.png`\n",
    "  - `figures/top_sites_latest_day_test.png`\n",
    "\n",
    "**Why this matters:**\n",
    "- This creates a **clean “report artifact” set** suitable for GitHub review, hiring managers, or stakeholders, while keeping the large raw/processed datasets out of version control (consistent with your `.gitignore`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99af3a9d-8cad-4d87-93c9-94b011c20157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved paths:\n",
      "  ROOT       : /home/parallels/projects/gmp-packaging-risk-analytics\n",
      "  RS_DIR      : /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring\n",
      "  EXPORT_DIR  : /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z\n",
      "  LATEST_DIR  : /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/latest_test\n",
      "  REPORTS_DIR : /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test\n",
      "  REPORT_MD   : /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/DASHBOARD_REPORT_TEST.md\n",
      "\n",
      "Wrote reports index: /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test/README.md\n",
      "Updated repo exports index: /home/parallels/projects/gmp-packaging-risk-analytics/EXPORTS.json\n",
      "Updated run manifest with report pointers: /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json\n",
      "\n",
      "Preview: reports index (tail)\n",
      "\n",
      "- **Top sites (latest day):** `dashboard_top_sites_latest_day_test.csv`\n",
      "- **Top lines (latest day):** `dashboard_top_lines_latest_day_test.csv`\n",
      "- **Daily trend (last 14 days):** `dashboard_daily_trend_last14_test.csv`\n",
      "- **KPIs / provenance:** `dashboard_kpis_test.json`\n",
      "- **API-ready payload:** `dashboard_payload_test.json`\n",
      "\n",
      "## Figures\n",
      "- `figures/daily_risk_trend_test.png`\n",
      "- `figures/daily_positive_hours_test.png`\n",
      "- `figures/alert_queue_latest_day_test.png`\n",
      "- `figures/top_sites_latest_day_test.png`\n",
      "\n",
      "## Notes\n",
      "- TEST outputs are produced from a grouped-by-asset split and a Top-5 assets/day alert budget.\n",
      "- For full raw/processed artifacts, see the run directory under `data/processed/risk_scoring/<export_run_id>/` (not committed).\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 10 — GitHub-friendly reports index + EXPORTS pointers (TEST)\n",
    "#   - Create a small README index under reports/ so GitHub viewers can navigate outputs\n",
    "#   - Update EXPORTS.json (repo root) with links to the latest TEST dashboard report snapshot\n",
    "#   - Update the run manifest (DASHBOARD_EXPORTS.json) with the report locations\n",
    "#============================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Resolve paths robustly (works after kernel restart)\n",
    "# -----------------------------\n",
    "ROOT = Path(globals().get(\"ROOT\", Path.cwd())).resolve()\n",
    "\n",
    "# Preferred: variables created in earlier cells\n",
    "EXPORT_DIR = Path(globals().get(\"EXPORT_DIR\", \"\")) if globals().get(\"EXPORT_DIR\") else None\n",
    "LATEST_DIR = Path(globals().get(\"LATEST_DIR\", \"\")) if globals().get(\"LATEST_DIR\") else None\n",
    "\n",
    "# Fallbacks if the kernel was restarted and variables are missing\n",
    "RS_DIR = Path(globals().get(\"RS_DIR\", ROOT / \"data\" / \"processed\" / \"risk_scoring\")).resolve()\n",
    "\n",
    "if LATEST_DIR is None or not LATEST_DIR.exists():\n",
    "    LATEST_DIR = (RS_DIR / \"latest_test\").resolve()\n",
    "\n",
    "if EXPORT_DIR is None or not EXPORT_DIR.exists():\n",
    "    # Use pointer JSON in latest_test to locate the last export_run_id\n",
    "    pointer = LATEST_DIR / \"LATEST_TEST.json\"\n",
    "    if pointer.exists():\n",
    "        try:\n",
    "            ptr = json.loads(pointer.read_text())\n",
    "            # common patterns: ptr[\"export_run_id\"] or ptr[\"export_dir\"]\n",
    "            if isinstance(ptr, dict) and \"export_dir\" in ptr:\n",
    "                EXPORT_DIR = Path(ptr[\"export_dir\"]).resolve()\n",
    "            elif isinstance(ptr, dict) and \"export_run_id\" in ptr:\n",
    "                EXPORT_DIR = (RS_DIR / str(ptr[\"export_run_id\"])).resolve()\n",
    "        except Exception:\n",
    "            EXPORT_DIR = None\n",
    "\n",
    "# Final sanity checks\n",
    "REPORTS_DIR = (ROOT / \"reports\" / \"risk_scoring\" / \"latest_test\").resolve()\n",
    "REPORT_MD = REPORTS_DIR / \"DASHBOARD_REPORT_TEST.md\"\n",
    "RUN_MANIFEST = (EXPORT_DIR / \"DASHBOARD_EXPORTS.json\").resolve() if EXPORT_DIR else None\n",
    "ROOT_EXPORTS = (ROOT / \"EXPORTS.json\").resolve()\n",
    "\n",
    "assert REPORTS_DIR.exists(), f\"Missing reports snapshot dir: {REPORTS_DIR}\"\n",
    "assert REPORT_MD.exists(), f\"Missing report markdown: {REPORT_MD}\"\n",
    "assert LATEST_DIR.exists(), f\"Missing latest_test dir: {LATEST_DIR}\"\n",
    "if RUN_MANIFEST:\n",
    "    assert RUN_MANIFEST.exists(), f\"Missing run manifest: {RUN_MANIFEST}\"\n",
    "\n",
    "print(\"Resolved paths:\")\n",
    "print(\"  ROOT       :\", ROOT)\n",
    "print(\"  RS_DIR      :\", RS_DIR)\n",
    "print(\"  EXPORT_DIR  :\", EXPORT_DIR if EXPORT_DIR else \"(unknown)\")\n",
    "print(\"  LATEST_DIR  :\", LATEST_DIR)\n",
    "print(\"  REPORTS_DIR :\", REPORTS_DIR)\n",
    "print(\"  REPORT_MD   :\", REPORT_MD)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Write a lightweight reports index README (GitHub navigation helper)\n",
    "# -----------------------------\n",
    "reports_index = REPORTS_DIR / \"README.md\"\n",
    "\n",
    "# Keep it short and “GitHub-first”: explain what’s inside + link files relative to this folder.\n",
    "index_lines = [\n",
    "    \"# Risk Scoring Dashboard — Latest TEST Snapshot\",\n",
    "    \"\",\n",
    "    \"This folder contains a **git-friendly snapshot** of the latest TEST exports from the panel risk-scoring pipeline.\",\n",
    "    \"It is intentionally small so it can be reviewed on GitHub without committing the full processed dataset.\",\n",
    "    \"\",\n",
    "    \"## Quick links\",\n",
    "    f\"- **Dashboard Report (Markdown):** `{REPORT_MD.name}`\",\n",
    "    f\"- **Alert queue (latest day):** `dashboard_alert_queue_latest_day_test.csv`\",\n",
    "    f\"- **Top sites (latest day):** `dashboard_top_sites_latest_day_test.csv`\",\n",
    "    f\"- **Top lines (latest day):** `dashboard_top_lines_latest_day_test.csv`\",\n",
    "    f\"- **Daily trend (last 14 days):** `dashboard_daily_trend_last14_test.csv`\",\n",
    "    f\"- **KPIs / provenance:** `dashboard_kpis_test.json`\",\n",
    "    f\"- **API-ready payload:** `dashboard_payload_test.json`\",\n",
    "    \"\",\n",
    "    \"## Figures\",\n",
    "    \"- `figures/daily_risk_trend_test.png`\",\n",
    "    \"- `figures/daily_positive_hours_test.png`\",\n",
    "    \"- `figures/alert_queue_latest_day_test.png`\",\n",
    "    \"- `figures/top_sites_latest_day_test.png`\",\n",
    "    \"\",\n",
    "    \"## Notes\",\n",
    "    \"- TEST outputs are produced from a grouped-by-asset split and a Top-5 assets/day alert budget.\",\n",
    "    \"- For full raw/processed artifacts, see the run directory under `data/processed/risk_scoring/<export_run_id>/` (not committed).\",\n",
    "]\n",
    "reports_index.write_text(\"\\n\".join(index_lines))\n",
    "print(\"\\nWrote reports index:\", reports_index)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Utility: safe JSON loader that tolerates old/empty files\n",
    "# -----------------------------\n",
    "def safe_load_json(path: Path):\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(path.read_text())\n",
    "        # We expect dicts for manifests/exports; if something else, wrap it safely.\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "        return {\"_raw\": obj}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def safe_write_json(path: Path, obj: dict):\n",
    "    path.write_text(json.dumps(obj, indent=2, sort_keys=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Update repo-level EXPORTS.json with report pointers (GitHub-safe)\n",
    "# -----------------------------\n",
    "exports = safe_load_json(ROOT_EXPORTS)\n",
    "exports.setdefault(\"risk_scoring\", {})\n",
    "exports[\"risk_scoring\"].setdefault(\"latest_test\", {})\n",
    "\n",
    "# Store paths relative to repo root when possible (reads better on GitHub)\n",
    "def rel_to_root(p: Path) -> str:\n",
    "    try:\n",
    "        return str(p.resolve().relative_to(ROOT))\n",
    "    except Exception:\n",
    "        return str(p)\n",
    "\n",
    "exports[\"risk_scoring\"][\"latest_test\"].update({\n",
    "    \"reports_dir\": rel_to_root(REPORTS_DIR),\n",
    "    \"report_md\": rel_to_root(REPORT_MD),\n",
    "    \"reports_index_md\": rel_to_root(reports_index),\n",
    "    \"latest_published_dir\": rel_to_root(LATEST_DIR),\n",
    "    \"updated_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "})\n",
    "\n",
    "safe_write_json(ROOT_EXPORTS, exports)\n",
    "print(\"Updated repo exports index:\", ROOT_EXPORTS)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Update run manifest with report snapshot pointers (nice provenance)\n",
    "# -----------------------------\n",
    "if RUN_MANIFEST:\n",
    "    manifest = safe_load_json(RUN_MANIFEST)\n",
    "\n",
    "    # Ensure the manifest has the expected dict structure\n",
    "    if not isinstance(manifest, dict):\n",
    "        manifest = {\"_raw\": manifest}\n",
    "\n",
    "    manifest.setdefault(\"reports\", {})\n",
    "    manifest[\"reports\"].update({\n",
    "        \"report_snapshot_dir\": str(REPORTS_DIR),\n",
    "        \"dashboard_report_test_md\": str(REPORT_MD),\n",
    "        \"report_index_md\": str(reports_index),\n",
    "        \"published_latest_dir\": str(LATEST_DIR),\n",
    "        \"updated_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    })\n",
    "\n",
    "    safe_write_json(RUN_MANIFEST, manifest)\n",
    "    print(\"Updated run manifest with report pointers:\", RUN_MANIFEST)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Quick tail preview (helps confirm in-notebook)\n",
    "# -----------------------------\n",
    "print(\"\\nPreview: reports index (tail)\\n\")\n",
    "print(\"\\n\".join(reports_index.read_text().splitlines()[-15:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783eea03-9854-454f-96e6-0bc2f5cf1f8a",
   "metadata": {},
   "source": [
    "### What Cell 10 Just Did — GitHub-friendly report index + export pointers (TEST)\n",
    "\n",
    "This cell **finalized the “human-readable / GitHub-readable” layer** of the risk-scoring outputs by creating a stable navigation entrypoint and wiring it into the repo-level export index.\n",
    "\n",
    "**Key actions and outputs:**\n",
    "- **Resolved all key paths** (root repo, risk_scoring folder, run export directory, latest published directory, and report snapshot directory), confirming the run we’re indexing is:\n",
    "  - `EXPORT_DIR`: `/home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/20251220T223649Z`\n",
    "  - `LATEST_DIR`: `/home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/latest_test`\n",
    "  - `REPORTS_DIR`: `/home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test`\n",
    "  - `REPORT_MD`: `.../reports/risk_scoring/latest_test/DASHBOARD_REPORT_TEST.md`\n",
    "\n",
    "- **Wrote a GitHub-friendly README index** inside the report snapshot folder:\n",
    "  - `reports/risk_scoring/latest_test/README.md`\n",
    "  This README provides quick links to the report, CSV outputs, and figure files so someone browsing the repo can understand and navigate the deliverables without needing the full processed dataset.\n",
    "\n",
    "- **Updated the repo-wide exports registry** to include pointers for the “latest TEST” dashboard snapshot:\n",
    "  - `EXPORTS.json`\n",
    "  This makes the dashboard report discoverable from a single canonical index at the repo root.\n",
    "\n",
    "- **Updated the run manifest** to include report snapshot locations and publication pointers (provenance / traceability):\n",
    "  - `data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json`\n",
    "\n",
    "**Why this matters:**\n",
    "- We now have a clean separation between:\n",
    "  - **large, non-committed artifacts** in `data/processed/...`, and\n",
    "  - **small, GitHub-friendly “report snapshots”** in `reports/...` that are easy to review and share.\n",
    "- Anyone reviewing the repo can start at `reports/risk_scoring/latest_test/README.md` and follow links to the report, figures, and tables.\n",
    "\n",
    "**Saved/Updated artifacts (confirmed by the cell output):**\n",
    "- Created: `reports/risk_scoring/latest_test/README.md`\n",
    "- Updated: `EXPORTS.json`\n",
    "- Updated: `data/processed/risk_scoring/20251220T223649Z/DASHBOARD_EXPORTS.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e23d786-d6a8-4a93-bcd9-60b92d51db80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved paths:\n",
      "  ROOT       : /home/parallels/projects/gmp-packaging-risk-analytics\n",
      "  RS_DIR     : /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring\n",
      "  LATEST_DIR : /home/parallels/projects/gmp-packaging-risk-analytics/data/processed/risk_scoring/latest_test\n",
      "  REPORTS_DIR: /home/parallels/projects/gmp-packaging-risk-analytics/reports/risk_scoring/latest_test\n",
      "\n",
      "Deliverables (sizes):\n",
      "  - reports/risk_scoring/latest_test/DASHBOARD_REPORT_TEST.md  (3.26 KB)\n",
      "  - reports/risk_scoring/latest_test/README.md  (1.13 KB)\n",
      "  - data/processed/risk_scoring/latest_test/dashboard_payload_test.json  (11.41 KB)\n",
      "  - data/processed/risk_scoring/latest_test/dashboard_kpis_test.json  (0.54 KB)\n",
      "  - data/processed/risk_scoring/latest_test/LATEST_TEST.json  (0.28 KB)\n",
      "  - EXPORTS.json  (2.65 KB)\n",
      "\n",
      "Payload sanity preview:\n",
      "  keys: ['alert_queue_latest_day', 'created_utc', 'daily_trend_last14', 'kind', 'kpis', 'latest_day', 'notes', 'published_latest_dir', 'source_export_run_id', 'source_panel_run_dir', 'top_lines_latest_day', 'top_sites_latest_day'] ...\n",
      "  latest_day: None\n",
      "  alert_queue rows: n/a\n",
      "\n",
      "What to commit (recommended):\n",
      "  - 03_risk_scoring_and_dashboard_exports.ipynb\n",
      "  - reports/risk_scoring/latest_test/ (README + DASHBOARD_REPORT_TEST.md + figures + CSVs copied for reporting)\n",
      "  - EXPORTS.json\n",
      "\n",
      "What NOT to commit:\n",
      "  - data/processed/** (large run artifacts; ignored by .gitignore)\n",
      "\n",
      "Suggested git commands:\n",
      "  git status --porcelain\n",
      "  git add 03_risk_scoring_and_dashboard_exports.ipynb EXPORTS.json reports/risk_scoring/latest_test/\n",
      "  git commit -m \"Add risk scoring dashboard exports (TEST) + report index\"\n",
      "  git push\n",
      "\n",
      "Wrote: reports/risk_scoring/latest_test/COMMIT_NOTES.md\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# Cell 11 — Wrap-up checklist: verify deliverables + print “what to commit” (GitHub-safe pointers)\n",
    "#============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "RS_DIR = ROOT / \"data\" / \"processed\" / \"risk_scoring\"\n",
    "REPORTS_DIR = ROOT / \"reports\" / \"risk_scoring\" / \"latest_test\"\n",
    "LATEST_DIR = RS_DIR / \"latest_test\"\n",
    "EXPORTS_JSON = ROOT / \"EXPORTS.json\"\n",
    "\n",
    "REPORT_MD = REPORTS_DIR / \"DASHBOARD_REPORT_TEST.md\"\n",
    "REPORT_INDEX = REPORTS_DIR / \"README.md\"\n",
    "PAYLOAD = LATEST_DIR / \"dashboard_payload_test.json\"\n",
    "KPIS = LATEST_DIR / \"dashboard_kpis_test.json\"\n",
    "LATEST_POINTER = LATEST_DIR / \"LATEST_TEST.json\"\n",
    "RUN_MANIFEST = None  # best-effort: discover from LATEST_TEST.json if present\n",
    "\n",
    "print(\"Resolved paths:\")\n",
    "print(\"  ROOT       :\", ROOT)\n",
    "print(\"  RS_DIR     :\", RS_DIR)\n",
    "print(\"  LATEST_DIR :\", LATEST_DIR)\n",
    "print(\"  REPORTS_DIR:\", REPORTS_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Existence checks (fail fast if something important is missing)\n",
    "# -----------------------------\n",
    "must_exist = [\n",
    "    REPORTS_DIR,\n",
    "    REPORT_MD,\n",
    "    REPORT_INDEX,\n",
    "    LATEST_DIR,\n",
    "    PAYLOAD,\n",
    "    KPIS,\n",
    "    LATEST_POINTER,\n",
    "    EXPORTS_JSON,\n",
    "]\n",
    "\n",
    "missing = [p for p in must_exist if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\"Missing expected deliverables:\\n\" + \"\\n\".join([f\"- {p}\" for p in missing]))\n",
    "\n",
    "# Try to locate the specific run manifest from the LATEST pointer (best-effort)\n",
    "try:\n",
    "    latest_meta = json.loads(LATEST_POINTER.read_text())\n",
    "    run_manifest_path = latest_meta.get(\"run_manifest_path\") or latest_meta.get(\"run_manifest\")  # tolerate key changes\n",
    "    if run_manifest_path:\n",
    "        RUN_MANIFEST = Path(run_manifest_path)\n",
    "except Exception:\n",
    "    RUN_MANIFEST = None\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Size + quick preview (helps ensure we’re not committing huge binaries)\n",
    "# -----------------------------\n",
    "def file_size_kb(p: Path) -> float:\n",
    "    return round(p.stat().st_size / 1024.0, 2)\n",
    "\n",
    "print(\"\\nDeliverables (sizes):\")\n",
    "for p in [REPORT_MD, REPORT_INDEX, PAYLOAD, KPIS, LATEST_POINTER, EXPORTS_JSON]:\n",
    "    print(f\"  - {p.relative_to(ROOT)}  ({file_size_kb(p)} KB)\")\n",
    "\n",
    "# Show a tiny preview of the payload (API wiring sanity)\n",
    "payload_obj = json.loads(PAYLOAD.read_text())\n",
    "print(\"\\nPayload sanity preview:\")\n",
    "print(\"  keys:\", sorted(list(payload_obj.keys()))[:20], \"...\")\n",
    "print(\"  latest_day:\", payload_obj.get(\"latest_day_utc\"))\n",
    "print(\"  alert_queue rows:\", len(payload_obj.get(\"alert_queue\", [])) if isinstance(payload_obj.get(\"alert_queue\"), list) else \"n/a\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) “What to commit” guidance (print only; you run git commands in terminal)\n",
    "# -----------------------------\n",
    "print(\"\\nWhat to commit (recommended):\")\n",
    "print(\"  - 03_risk_scoring_and_dashboard_exports.ipynb\")\n",
    "print(\"  - reports/risk_scoring/latest_test/ (README + DASHBOARD_REPORT_TEST.md + figures + CSVs copied for reporting)\")\n",
    "print(\"  - EXPORTS.json\")\n",
    "print(\"\\nWhat NOT to commit:\")\n",
    "print(\"  - data/processed/** (large run artifacts; ignored by .gitignore)\")\n",
    "\n",
    "print(\"\\nSuggested git commands:\")\n",
    "print(\"  git status --porcelain\")\n",
    "print(\"  git add 03_risk_scoring_and_dashboard_exports.ipynb EXPORTS.json reports/risk_scoring/latest_test/\")\n",
    "print(\"  git commit -m \\\"Add risk scoring dashboard exports (TEST) + report index\\\"\")\n",
    "print(\"  git push\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Optional: write a small commit-helper pointer file in reports/ (GitHub readability)\n",
    "# -----------------------------\n",
    "commit_note_path = REPORTS_DIR / \"COMMIT_NOTES.md\"\n",
    "commit_note = f\"\"\"# Commit Notes — Risk Scoring Dashboard (TEST)\n",
    "\n",
    "This folder contains the **GitHub-friendly snapshot** of the latest TEST dashboard exports.\n",
    "\n",
    "## Key files\n",
    "- `README.md` — index of outputs\n",
    "- `DASHBOARD_REPORT_TEST.md` — narrative report\n",
    "- `dashboard_payload_test.json` — API-ready payload (also mirrored under `data/processed/risk_scoring/latest_test/`)\n",
    "- Figures are under `figures/`\n",
    "\n",
    "## Provenance\n",
    "- Latest pointer: `{LATEST_POINTER}`\n",
    "- Panel run dir referenced in payload/KPIs: `{payload_obj.get(\"panel_run_dir\")}`\n",
    "\"\"\"\n",
    "commit_note_path.write_text(commit_note)\n",
    "print(\"\\nWrote:\", commit_note_path.relative_to(ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff733cd-96b1-4c19-9cb8-a9cac1adb3d9",
   "metadata": {},
   "source": [
    "### What Cell 11 Just Did\n",
    "\n",
    "**Purpose**\n",
    "- Performed a final “wrap-up” verification that the **risk scoring TEST dashboard deliverables** exist in the expected locations.\n",
    "- Printed **GitHub-safe commit guidance** (what to commit vs. what to keep out of git).\n",
    "- Wrote a small helper note file: `reports/risk_scoring/latest_test/COMMIT_NOTES.md`.\n",
    "\n",
    "**Key paths resolved**\n",
    "- **Repo root:** `/home/parallels/projects/gmp-packaging-risk-analytics`\n",
    "- **Latest TEST exports (data, not committed):** `data/processed/risk_scoring/latest_test/`\n",
    "- **Latest TEST reports (GitHub-friendly):** `reports/risk_scoring/latest_test/`\n",
    "\n",
    "**Verified deliverables + sizes**\n",
    "- `reports/risk_scoring/latest_test/DASHBOARD_REPORT_TEST.md` (~3.26 KB)\n",
    "- `reports/risk_scoring/latest_test/README.md` (~1.13 KB)\n",
    "- `data/processed/risk_scoring/latest_test/dashboard_payload_test.json` (~11.41 KB)\n",
    "- `data/processed/risk_scoring/latest_test/dashboard_kpis_test.json` (~0.54 KB)\n",
    "- `data/processed/risk_scoring/latest_test/LATEST_TEST.json` (~0.28 KB)\n",
    "- `EXPORTS.json` (~2.65 KB)\n",
    "\n",
    "**Important note (payload preview mismatch)**\n",
    "- The payload keys indicate the schema is:\n",
    "  - `latest_day` (not `latest_day_utc`)\n",
    "  - `alert_queue_latest_day` (not `alert_queue`)\n",
    "- The preview printed:\n",
    "  - `latest_day: None`\n",
    "  - `alert_queue rows: n/a`\n",
    "- That means **either**:\n",
    "  1) The preview code is checking the wrong keys (likely), **and/or**\n",
    "  2) The payload generator stored `latest_day` as `None` (needs a small fix if you plan to serve this via API).\n",
    "\n",
    "A quick sanity check you can run right now:\n",
    "- `payload[\"latest_day\"]`\n",
    "- `len(payload[\"alert_queue_latest_day\"])`\n",
    "- `len(payload[\"daily_trend_last14\"])`\n",
    "\n",
    "**GitHub commit guidance (printed)**\n",
    "- **Commit:**\n",
    "  - `03_risk_scoring_and_dashboard_exports.ipynb`\n",
    "  - `EXPORTS.json`\n",
    "  - `reports/risk_scoring/latest_test/` (index + report + figures + small CSV copies)\n",
    "- **Do NOT commit:**\n",
    "  - `data/processed/**` (large run artifacts; intentionally ignored)\n",
    "\n",
    "**Suggested terminal commands**\n",
    "- `git status --porcelain`\n",
    "- `git add 03_risk_scoring_and_dashboard_exports.ipynb EXPORTS.json reports/risk_scoring/latest_test/`\n",
    "- `git commit -m \"Add risk scoring dashboard exports (TEST) + report index\"`\n",
    "- `git push`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1825007-a60d-4b47-9fcd-35712be3e6f4",
   "metadata": {},
   "source": [
    "## Notebook Wrap-Up — `03_risk_scoring_and_dashboard_exports.ipynb`\n",
    "\n",
    "This notebook takes the **Panel (asset-hour) TEST scoring outputs** and converts them into **business-ready dashboard exports** under a strict **alerts budget of Top-5 assets/day**. It also generates lightweight charts, bundles the outputs, publishes a “latest_test” snapshot, and produces a human-readable report for GitHub.\n",
    "\n",
    "### What we produced\n",
    "- **Scored TEST table (asset-hour):** saved as parquet + CSV\n",
    "- **Alerts under budget:** Top-5 risky assets/day with:\n",
    "  - **when**: peak-risk hour (`ts_peak`)\n",
    "  - **why**: top coefficient contributions at that peak hour (push-to-1 vs push-to-0)\n",
    "- **Dashboard tables:** alert queue, top sites, top lines, daily trend, KPIs\n",
    "- **Figures:** daily risk trend, positive-hours trend, latest-day alert queue, top sites\n",
    "- **Bundle:** ZIP containing the dashboard tables + figures\n",
    "- **Stable publish target:** `data/processed/risk_scoring/latest_test/`\n",
    "- **GitHub-friendly report target:** `reports/risk_scoring/latest_test/`\n",
    "\n",
    "### Where the “latest” outputs live\n",
    "- **API-ready + operational files (not committed):**\n",
    "  - `data/processed/risk_scoring/latest_test/`\n",
    "- **GitHub-readable report and index (committed):**\n",
    "  - `reports/risk_scoring/latest_test/`\n",
    "\n",
    "### Operational note: payload field check\n",
    "Before wiring the API endpoint, confirm the payload includes a valid latest day and queue rows:\n",
    "- `latest_day` should be a date (e.g., `2025-12-11`)\n",
    "- `alert_queue_latest_day` should be a list with 5 rows\n",
    "\n",
    "If `latest_day` is `None`, we’ll patch the payload builder to set it from:\n",
    "- `trend[\"date_utc\"].max()` (preferred), or\n",
    "- `alert_queue[\"date_utc\"].max()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba8fe4-b3ed-41c0-8676-e61bdfc5585c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gmp-packaging-risk-analytics)",
   "language": "python",
   "name": "gmp-packaging-risk-analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
